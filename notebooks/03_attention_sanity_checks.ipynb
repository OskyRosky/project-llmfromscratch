{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61cfbf2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sultan/DataScience/LLM-From-Scratch-Project/.venv/bin/python\n",
      "2.9.1\n"
     ]
    }
   ],
   "source": [
    "import sys, torch\n",
    "print(sys.executable)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "704d8576",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mattention\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      4\u001b[39m     create_causal_mask,\n\u001b[32m      5\u001b[39m     scaled_dot_product_attention,\n\u001b[32m      6\u001b[39m     MultiHeadAttention,\n\u001b[32m      7\u001b[39m )\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     10\u001b[39m     TokenEmbedding,\n\u001b[32m     11\u001b[39m     PositionalEmbedding,\n\u001b[32m     12\u001b[39m     FeedForward,\n\u001b[32m     13\u001b[39m     LayerNorm,\n\u001b[32m     14\u001b[39m )\n\u001b[32m     16\u001b[39m torch.__version__\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from src.model.attention import (\n",
    "    create_causal_mask,\n",
    "    scaled_dot_product_attention,\n",
    "    MultiHeadAttention,\n",
    ")\n",
    "\n",
    "from src.model.layers import (\n",
    "    TokenEmbedding,\n",
    "    PositionalEmbedding,\n",
    "    FeedForward,\n",
    "    LayerNorm,\n",
    ")\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50671fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f474d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, num_heads, seq_len, head_dim = 1, 1, 4, 2\n",
    "\n",
    "q = torch.randn(batch_size, num_heads, seq_len, head_dim)\n",
    "k = torch.randn(batch_size, num_heads, seq_len, head_dim)\n",
    "v = torch.randn(batch_size, num_heads, seq_len, head_dim)\n",
    "\n",
    "mask = create_causal_mask(seq_len, device=q.device)\n",
    "\n",
    "out, attn = scaled_dot_product_attention(q, k, v, mask=mask)\n",
    "\n",
    "print(\"q shape:\", q.shape)\n",
    "print(\"k shape:\", k.shape)\n",
    "print(\"v shape:\", v.shape)\n",
    "print(\"out shape:\", out.shape)\n",
    "print(\"attn shape:\", attn.shape)\n",
    "print(\"\\nCausal mask (0 = futuro bloqueado):\")\n",
    "print(mask[0, 0].int())\n",
    "print(\"\\nAttention matrix (head 0):\")\n",
    "print(attn[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322d6369",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, seq_len, embed_dim, num_heads = 2, 5, 8, 2\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, embed_dim)\n",
    "mha = MultiHeadAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "\n",
    "mask = create_causal_mask(seq_len, device=x.device)\n",
    "\n",
    "out, attn = mha(x, mask=mask)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", out.shape)\n",
    "print(\"Attention shape:\", attn.shape)\n",
    "print(\"\\nAttention matrix (batch 0, head 0):\")\n",
    "print(attn[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f119f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50\n",
    "max_seq_len = 16\n",
    "embed_dim = 8\n",
    "batch_size, seq_len = 2, 10\n",
    "\n",
    "ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "tok_emb = TokenEmbedding(vocab_size, embed_dim)\n",
    "pos_emb = PositionalEmbedding(max_seq_len, embed_dim)\n",
    "\n",
    "t = tok_emb(ids)\n",
    "p = pos_emb(ids)\n",
    "s = t + p\n",
    "\n",
    "print(\"Token emb shape:\", t.shape)\n",
    "print(\"Pos emb shape:\", p.shape)\n",
    "print(\"Sum shape:\", s.shape)\n",
    "print(\"\\nExample token embedding[0,0]:\", t[0, 0])\n",
    "print(\"Example pos embedding[0,0]:\", p[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc33562f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, seq_len, d_model = 2, 5, 8\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "ff = FeedForward(d_model)\n",
    "ln = LayerNorm(d_model)\n",
    "\n",
    "y = ff(x)\n",
    "z = ln(y)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"FFN output shape:\", y.shape)\n",
    "print(\"LayerNorm output shape:\", z.shape)\n",
    "\n",
    "# Opcional: ver medias y desviaciones por posiciÃ³n\n",
    "print(\"\\nMean over last dim before LN (first token):\", y[0, 0].mean().item())\n",
    "print(\"Std over last dim before LN (first token):\", y[0, 0].std(unbiased=False).item())\n",
    "\n",
    "print(\"\\nMean over last dim after LN (first token):\", z[0, 0].mean().item())\n",
    "print(\"Std over last dim after LN (first token):\", z[0, 0].std(unbiased=False).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fdc1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini pipeline: ids -> embeddings -> MHA -> FFN + LN\n",
    "\n",
    "vocab_size = 50\n",
    "max_seq_len = 16\n",
    "embed_dim = 8\n",
    "num_heads = 2\n",
    "batch_size, seq_len = 2, 10\n",
    "\n",
    "ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "tok_emb = TokenEmbedding(vocab_size, embed_dim)\n",
    "pos_emb = PositionalEmbedding(max_seq_len, embed_dim)\n",
    "mha = MultiHeadAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "ff = FeedForward(embed_dim)\n",
    "ln1 = LayerNorm(embed_dim)\n",
    "ln2 = LayerNorm(embed_dim)\n",
    "\n",
    "x = tok_emb(ids) + pos_emb(ids)\n",
    "\n",
    "mask = create_causal_mask(seq_len, device=x.device)\n",
    "\n",
    "att_out, att_weights = mha(x, mask=mask)\n",
    "x = x + att_out            # residual 1\n",
    "x = ln1(x)\n",
    "\n",
    "ff_out = ff(x)\n",
    "x = x + ff_out             # residual 2\n",
    "x = ln2(x)\n",
    "\n",
    "print(\"Final output shape:\", x.shape)\n",
    "print(\"Attention weights shape:\", att_weights.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM-From-Scratch (venv)",
   "language": "python",
   "name": "llm-from-scratch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
