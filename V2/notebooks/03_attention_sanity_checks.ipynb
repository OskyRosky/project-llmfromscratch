{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "704d8576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python exe: /Users/sultan/DataScience/LLM-From-Scratch-Project/.venv/bin/python\n",
      "Torch version: 2.9.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Añadimos la raíz del proyecto al path de Python\n",
    "PROJECT_ROOT = \"/Users/sultan/DataScience/LLM-From-Scratch-Project\"\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "import torch\n",
    "\n",
    "from src.model.attention import (\n",
    "    create_causal_mask,\n",
    "    scaled_dot_product_attention,\n",
    "    MultiHeadAttention,\n",
    ")\n",
    "\n",
    "from src.model.layers import (\n",
    "    TokenEmbedding,\n",
    "    PositionalEmbedding,\n",
    "    FeedForward,\n",
    "    LayerNorm,\n",
    ")\n",
    "\n",
    "print(\"Python exe:\", sys.executable)\n",
    "print(\"Torch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1f474d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q shape: torch.Size([1, 1, 4, 2])\n",
      "k shape: torch.Size([1, 1, 4, 2])\n",
      "v shape: torch.Size([1, 1, 4, 2])\n",
      "out shape: torch.Size([1, 1, 4, 2])\n",
      "attn shape: torch.Size([1, 1, 4, 4])\n",
      "\n",
      "Causal mask (0 = futuro bloqueado):\n",
      "tensor([[1, 0, 0, 0],\n",
      "        [1, 1, 0, 0],\n",
      "        [1, 1, 1, 0],\n",
      "        [1, 1, 1, 1]], dtype=torch.int32)\n",
      "\n",
      "Attention matrix (head 0):\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5846, 0.4154, 0.0000, 0.0000],\n",
      "        [0.2029, 0.6149, 0.1822, 0.0000],\n",
      "        [0.1886, 0.0650, 0.1352, 0.6112]])\n"
     ]
    }
   ],
   "source": [
    "batch_size, num_heads, seq_len, head_dim = 1, 1, 4, 2\n",
    "\n",
    "q = torch.randn(batch_size, num_heads, seq_len, head_dim)\n",
    "k = torch.randn(batch_size, num_heads, seq_len, head_dim)\n",
    "v = torch.randn(batch_size, num_heads, seq_len, head_dim)\n",
    "\n",
    "mask = create_causal_mask(seq_len, device=q.device)\n",
    "\n",
    "out, attn = scaled_dot_product_attention(q, k, v, mask=mask)\n",
    "\n",
    "print(\"q shape:\", q.shape)\n",
    "print(\"k shape:\", k.shape)\n",
    "print(\"v shape:\", v.shape)\n",
    "print(\"out shape:\", out.shape)\n",
    "print(\"attn shape:\", attn.shape)\n",
    "print(\"\\nCausal mask (0 = futuro bloqueado):\")\n",
    "print(mask[0, 0].int())\n",
    "print(\"\\nAttention matrix (head 0):\")\n",
    "print(attn[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "322d6369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 5, 8])\n",
      "Output shape: torch.Size([2, 5, 8])\n",
      "Attention shape: torch.Size([2, 2, 5, 5])\n",
      "\n",
      "Attention matrix (batch 0, head 0):\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3857, 0.6143, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2425, 0.3793, 0.3782, 0.0000, 0.0000],\n",
      "        [0.2708, 0.1764, 0.2611, 0.2917, 0.0000],\n",
      "        [0.2018, 0.2011, 0.1663, 0.2144, 0.2164]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size, seq_len, embed_dim, num_heads = 2, 5, 8, 2\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, embed_dim)\n",
    "mha = MultiHeadAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "\n",
    "mask = create_causal_mask(seq_len, device=x.device)\n",
    "\n",
    "out, attn = mha(x, mask=mask)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", out.shape)\n",
    "print(\"Attention shape:\", attn.shape)\n",
    "print(\"\\nAttention matrix (batch 0, head 0):\")\n",
    "print(attn[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3f119f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token emb shape: torch.Size([2, 10, 8])\n",
      "Pos emb shape: torch.Size([2, 10, 8])\n",
      "Sum shape: torch.Size([2, 10, 8])\n",
      "\n",
      "Example token embedding[0,0]: tensor([ 1.3288, -2.4420,  1.1842, -0.8649, -6.8487,  0.9799,  0.6968, -0.2026],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Example pos embedding[0,0]: tensor([ 1.3486, -2.1934,  0.7030,  0.8502, -0.6056, -0.5264, -0.4830, -1.2382],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50\n",
    "max_seq_len = 16\n",
    "embed_dim = 8\n",
    "batch_size, seq_len = 2, 10\n",
    "\n",
    "ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "tok_emb = TokenEmbedding(vocab_size, embed_dim)\n",
    "pos_emb = PositionalEmbedding(max_seq_len, embed_dim)\n",
    "\n",
    "t = tok_emb(ids)\n",
    "p = pos_emb(ids)\n",
    "s = t + p\n",
    "\n",
    "print(\"Token emb shape:\", t.shape)\n",
    "print(\"Pos emb shape:\", p.shape)\n",
    "print(\"Sum shape:\", s.shape)\n",
    "print(\"\\nExample token embedding[0,0]:\", t[0, 0])\n",
    "print(\"Example pos embedding[0,0]:\", p[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc33562f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 5, 8])\n",
      "FFN output shape: torch.Size([2, 5, 8])\n",
      "LayerNorm output shape: torch.Size([2, 5, 8])\n",
      "\n",
      "Mean over last dim before LN (first token): -0.17933684587478638\n",
      "Std over last dim before LN (first token): 0.2778705060482025\n",
      "\n",
      "Mean over last dim after LN (first token): 1.4901161193847656e-08\n",
      "Std over last dim after LN (first token): 0.9999353289604187\n"
     ]
    }
   ],
   "source": [
    "batch_size, seq_len, d_model = 2, 5, 8\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "ff = FeedForward(d_model)\n",
    "ln = LayerNorm(d_model)\n",
    "\n",
    "y = ff(x)\n",
    "z = ln(y)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"FFN output shape:\", y.shape)\n",
    "print(\"LayerNorm output shape:\", z.shape)\n",
    "\n",
    "# Opcional: ver medias y desviaciones por posición\n",
    "print(\"\\nMean over last dim before LN (first token):\", y[0, 0].mean().item())\n",
    "print(\"Std over last dim before LN (first token):\", y[0, 0].std(unbiased=False).item())\n",
    "\n",
    "print(\"\\nMean over last dim after LN (first token):\", z[0, 0].mean().item())\n",
    "print(\"Std over last dim after LN (first token):\", z[0, 0].std(unbiased=False).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52fdc1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final output shape: torch.Size([2, 10, 8])\n",
      "Attention weights shape: torch.Size([2, 2, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "# Mini pipeline: ids -> embeddings -> MHA -> FFN + LN\n",
    "\n",
    "vocab_size = 50\n",
    "max_seq_len = 16\n",
    "embed_dim = 8\n",
    "num_heads = 2\n",
    "batch_size, seq_len = 2, 10\n",
    "\n",
    "ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "tok_emb = TokenEmbedding(vocab_size, embed_dim)\n",
    "pos_emb = PositionalEmbedding(max_seq_len, embed_dim)\n",
    "mha = MultiHeadAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "ff = FeedForward(embed_dim)\n",
    "ln1 = LayerNorm(embed_dim)\n",
    "ln2 = LayerNorm(embed_dim)\n",
    "\n",
    "x = tok_emb(ids) + pos_emb(ids)\n",
    "\n",
    "mask = create_causal_mask(seq_len, device=x.device)\n",
    "\n",
    "att_out, att_weights = mha(x, mask=mask)\n",
    "x = x + att_out            # residual 1\n",
    "x = ln1(x)\n",
    "\n",
    "ff_out = ff(x)\n",
    "x = x + ff_out             # residual 2\n",
    "x = ln2(x)\n",
    "\n",
    "print(\"Final output shape:\", x.shape)\n",
    "print(\"Attention weights shape:\", att_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3a121e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenizer sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a762ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers in /opt/homebrew/lib/python3.10/site-packages (0.22.1)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /opt/homebrew/lib/python3.10/site-packages (from tokenizers) (1.2.3)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (3.20.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (2025.12.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (0.28.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/sultan/Library/Python/3.10/lib/python/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (6.0.3)\n",
      "Requirement already satisfied: shellingham in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (1.5.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (4.67.1)\n",
      "Requirement already satisfied: typer-slim in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (0.21.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/sultan/Library/Python/3.10/lib/python/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (4.15.0)\n",
      "Requirement already satisfied: anyio in /opt/homebrew/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers) (4.12.0)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/homebrew/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/homebrew/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers) (0.16.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/sultan/Library/Python/3.10/lib/python/site-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers) (1.3.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/homebrew/lib/python3.10/site-packages (from typer-slim->huggingface-hub<2.0,>=0.16.4->tokenizers) (8.3.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.10/bin/python3.10 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf998838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/sultan/DataScience/LLM-From-Scratch-Project/V2/notebooks'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0a52a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec890a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tokenizers.Tokenizer, 'Tokenizer')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "TOKENIZER_PATH = \"../models/tokenizers/oscar_bpe_v2/tokenizer.json\"\n",
    "tokenizer = Tokenizer.from_file(TOKENIZER_PATH)\n",
    "\n",
    "type(tokenizer), tokenizer.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3154dbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "TOKENIZER_PATH = \"../models/tokenizers/oscar_bpe_v2/tokenizer.json\"\n",
    "print(\"Exists:\", os.path.exists(TOKENIZER_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6170389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tokenizers.Tokenizer, 'Tokenizer')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer), tokenizer.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "952bcd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_tokens: 29\n",
      "first_token_ids: [3669, 374, 299, 272, 289, 805, 767, 36, 655, 314, 70, 274, 272, 289, 2634, 767, 19, 425, 332, 3634, 272, 275, 461, 245, 1040, 1551, 321, 1468, 19]\n",
      "first_tokens: ['Un', 'Ġper', 'ro', 'Ġes', 'Ġun', 'Ġcan', 'ino', '?', 'ĠUn', 'Ġg', 'a', 'to', 'Ġes', 'Ġun', 'Ġfel', 'ino', '.', 'ĠLa', 'ĠT', 'ierra', 'Ġes', 'Ġel', 'Ġ3', 'er', 'Ġplan', 'eta', 'Ġdel', 'ĠSol', '.']\n",
      "decoded: Un perro es un canino? Un gato es un felino. La Tierra es el 3er planeta del Sol.\n"
     ]
    }
   ],
   "source": [
    "text = \"Un perro es un canino? Un gato es un felino. La Tierra es el 3er planeta del Sol.\"\n",
    "\n",
    "enc = tokenizer.encode(text)\n",
    "\n",
    "print(\"n_tokens:\", len(enc.ids))\n",
    "print(\"first_token_ids:\", enc.ids[:30])\n",
    "print(\"first_tokens:\", enc.tokens[:30])\n",
    "\n",
    "decoded = tokenizer.decode(enc.ids)\n",
    "print(\"decoded:\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d75bff42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook cwd: /Users/sultan/DataScience/LLM-From-Scratch-Project/V2/notebooks\n",
      "Search root : /Users/sultan/DataScience\n",
      "\n",
      "Found 151 candidate files\n",
      "\n",
      "   13.44 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/Base/models/checkpoints_oscar_long/gpt_char_best.pt\n",
      "   13.44 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/V2/models/checkpoints_oscar_long/gpt_char_best.pt\n",
      "   13.44 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/V1/models/checkpoints_oscar_long/gpt_char_best.pt\n",
      "   13.44 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/V2/models/checkpoints_oscar_debug/gpt_char_best.pt\n",
      "   13.44 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/V1/models/checkpoints_oscar_debug/gpt_char_best.pt\n",
      "   13.44 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/Base/models/checkpoints_oscar_debug/gpt_char_best.pt\n",
      "    5.99 MB  -  /Users/sultan/DataScience/RAG/Test RAG 2024/RAG/Tutorial/chroma_persistent_storage/34ef67d4-af93-4251-ba55-1df699839d5f/data_level0.bin\n",
      "    4.54 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/Base/models/checkpoints_oscar_long/gpt_char_cls_toy.pt\n",
      "    4.54 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/V2/models/checkpoints_oscar_long/gpt_char_cls_toy.pt\n",
      "    4.54 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/V1/models/checkpoints_oscar_long/gpt_char_cls_toy.pt\n",
      "    4.52 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/Base/models/checkpoints_oscar_long/gpt_char_cls_toy_v2.pt\n",
      "    4.52 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/V2/models/checkpoints_oscar_long/gpt_char_cls_toy_v2.pt\n",
      "    4.52 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/V1/models/checkpoints_oscar_long/gpt_char_cls_toy_v2.pt\n",
      "    4.52 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/Base/models/checkpoints_oscar_long/gpt_char_instructions.pt\n",
      "    4.52 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/V2/models/checkpoints_oscar_long/gpt_char_instructions.pt\n",
      "    4.52 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/V1/models/checkpoints_oscar_long/gpt_char_instructions.pt\n",
      "    3.06 MB  -  /Users/sultan/DataScience/RAG/RAG-Streamlit/chroma_db/8090dd62-8f90-467a-b5d1-f8b3a5cb1d9d/data_level0.bin\n",
      "    3.06 MB  -  /Users/sultan/DataScience/RAG/RAG_FROM_SCRATCH/chroma_db/8090dd62-8f90-467a-b5d1-f8b3a5cb1d9d/data_level0.bin\n",
      "    3.06 MB  -  /Users/sultan/DataScience/RAG/Test RAG 2024/RAG/TutPaulo/chroma_db/e161f161-6f29-4f75-b1cc-7aaae77d74ca/data_level0.bin\n",
      "    0.35 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/V1/models/checkpoints_tiny/gpt_char_best.pt\n",
      "    0.35 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/Base/models/checkpoints_tiny/gpt_char_best.pt\n",
      "    0.35 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/V2/models/checkpoints_tiny/gpt_char_best.pt\n",
      "    0.35 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/Base/models/checkpoints_test/toy_checkpoint.pt\n",
      "    0.35 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/V1/models/checkpoints_test/toy_checkpoint.pt\n",
      "    0.35 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/V2/models/checkpoints_test/toy_checkpoint.pt\n",
      "    0.07 MB  -  /Users/sultan/DataScience/RAG/RAG-Streamlit/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py\n",
      "    0.07 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/V1/.venv/lib/python3.13/site-packages/torch/utils/checkpoint.py\n",
      "    0.07 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/V2/.venv/lib/python3.13/site-packages/torch/utils/checkpoint.py\n",
      "    0.07 MB  -  /Users/sultan/DataScience/RAG/RAG_FROM_SCRATCH/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py\n",
      "    0.06 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/V2/.venv/lib/python3.13/site-packages/torch/utils/__pycache__/checkpoint.cpython-313.pyc\n",
      "    0.06 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/V1/.venv/lib/python3.13/site-packages/torch/utils/__pycache__/checkpoint.cpython-313.pyc\n",
      "    0.06 MB  -  /Users/sultan/DataScience/RAG/RAG_FROM_SCRATCH/.venv/lib/python3.12/site-packages/torch/utils/__pycache__/checkpoint.cpython-312.pyc\n",
      "    0.06 MB  -  /Users/sultan/DataScience/RAG/RAG-Streamlit/.venv/lib/python3.12/site-packages/torch/utils/__pycache__/checkpoint.cpython-312.pyc\n",
      "    0.06 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/Base/models/checkpoints_oscar_long/char_tokenizer.pt\n",
      "    0.06 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/V2/models/checkpoints_oscar_long/char_tokenizer.pt\n",
      "    0.06 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/Base/models/checkpoints_oscar_debug/char_tokenizer.pt\n",
      "    0.06 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/V2/models/checkpoints_oscar_debug/char_tokenizer.pt\n",
      "    0.06 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/V1/models/checkpoints_oscar_long/char_tokenizer.pt\n",
      "    0.06 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/V1/models/checkpoints_oscar_debug/char_tokenizer.pt\n",
      "    0.04 MB  -  /Users/sultan/DataScience/RAG/RAG-Streamlit/.venv/lib/python3.12/site-packages/transformers/models/megatron_gpt2/checkpoint_reshaping_and_interoperability.py\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "\n",
    "# subir 2 niveles: V2/notebooks -> V2 -> (repo root suele estar arriba de V2)\n",
    "repo_root = ROOT\n",
    "for _ in range(3):\n",
    "    repo_root = repo_root.parent\n",
    "\n",
    "print(\"Notebook cwd:\", ROOT)\n",
    "print(\"Search root :\", repo_root)\n",
    "\n",
    "patterns = [\"**/*.pt\", \"**/*.pth\", \"**/*.bin\", \"**/checkpoint*\", \"**/ckpt*\"]\n",
    "hits = []\n",
    "for p in patterns:\n",
    "    hits += list(repo_root.glob(p))\n",
    "\n",
    "hits = sorted(set(hits), key=lambda x: x.stat().st_size if x.exists() else 0, reverse=True)\n",
    "\n",
    "print(f\"\\nFound {len(hits)} candidate files\\n\")\n",
    "for h in hits[:40]:\n",
    "    size_mb = h.stat().st_size / (1024**2)\n",
    "    print(f\"{size_mb:8.2f} MB  -  {h}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ea0395",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
