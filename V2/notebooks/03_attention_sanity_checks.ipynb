{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "704d8576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python exe: /Users/sultan/DataScience/LLM-From-Scratch-Project/.venv/bin/python\n",
      "Torch version: 2.9.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Añadimos la raíz del proyecto al path de Python\n",
    "PROJECT_ROOT = \"/Users/sultan/DataScience/LLM-From-Scratch-Project\"\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "import torch\n",
    "\n",
    "from src.model.attention import (\n",
    "    create_causal_mask,\n",
    "    scaled_dot_product_attention,\n",
    "    MultiHeadAttention,\n",
    ")\n",
    "\n",
    "from src.model.layers import (\n",
    "    TokenEmbedding,\n",
    "    PositionalEmbedding,\n",
    "    FeedForward,\n",
    "    LayerNorm,\n",
    ")\n",
    "\n",
    "print(\"Python exe:\", sys.executable)\n",
    "print(\"Torch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1f474d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q shape: torch.Size([1, 1, 4, 2])\n",
      "k shape: torch.Size([1, 1, 4, 2])\n",
      "v shape: torch.Size([1, 1, 4, 2])\n",
      "out shape: torch.Size([1, 1, 4, 2])\n",
      "attn shape: torch.Size([1, 1, 4, 4])\n",
      "\n",
      "Causal mask (0 = futuro bloqueado):\n",
      "tensor([[1, 0, 0, 0],\n",
      "        [1, 1, 0, 0],\n",
      "        [1, 1, 1, 0],\n",
      "        [1, 1, 1, 1]], dtype=torch.int32)\n",
      "\n",
      "Attention matrix (head 0):\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5846, 0.4154, 0.0000, 0.0000],\n",
      "        [0.2029, 0.6149, 0.1822, 0.0000],\n",
      "        [0.1886, 0.0650, 0.1352, 0.6112]])\n"
     ]
    }
   ],
   "source": [
    "batch_size, num_heads, seq_len, head_dim = 1, 1, 4, 2\n",
    "\n",
    "q = torch.randn(batch_size, num_heads, seq_len, head_dim)\n",
    "k = torch.randn(batch_size, num_heads, seq_len, head_dim)\n",
    "v = torch.randn(batch_size, num_heads, seq_len, head_dim)\n",
    "\n",
    "mask = create_causal_mask(seq_len, device=q.device)\n",
    "\n",
    "out, attn = scaled_dot_product_attention(q, k, v, mask=mask)\n",
    "\n",
    "print(\"q shape:\", q.shape)\n",
    "print(\"k shape:\", k.shape)\n",
    "print(\"v shape:\", v.shape)\n",
    "print(\"out shape:\", out.shape)\n",
    "print(\"attn shape:\", attn.shape)\n",
    "print(\"\\nCausal mask (0 = futuro bloqueado):\")\n",
    "print(mask[0, 0].int())\n",
    "print(\"\\nAttention matrix (head 0):\")\n",
    "print(attn[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "322d6369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 5, 8])\n",
      "Output shape: torch.Size([2, 5, 8])\n",
      "Attention shape: torch.Size([2, 2, 5, 5])\n",
      "\n",
      "Attention matrix (batch 0, head 0):\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3857, 0.6143, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2425, 0.3793, 0.3782, 0.0000, 0.0000],\n",
      "        [0.2708, 0.1764, 0.2611, 0.2917, 0.0000],\n",
      "        [0.2018, 0.2011, 0.1663, 0.2144, 0.2164]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size, seq_len, embed_dim, num_heads = 2, 5, 8, 2\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, embed_dim)\n",
    "mha = MultiHeadAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "\n",
    "mask = create_causal_mask(seq_len, device=x.device)\n",
    "\n",
    "out, attn = mha(x, mask=mask)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", out.shape)\n",
    "print(\"Attention shape:\", attn.shape)\n",
    "print(\"\\nAttention matrix (batch 0, head 0):\")\n",
    "print(attn[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3f119f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token emb shape: torch.Size([2, 10, 8])\n",
      "Pos emb shape: torch.Size([2, 10, 8])\n",
      "Sum shape: torch.Size([2, 10, 8])\n",
      "\n",
      "Example token embedding[0,0]: tensor([ 1.3288, -2.4420,  1.1842, -0.8649, -6.8487,  0.9799,  0.6968, -0.2026],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Example pos embedding[0,0]: tensor([ 1.3486, -2.1934,  0.7030,  0.8502, -0.6056, -0.5264, -0.4830, -1.2382],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50\n",
    "max_seq_len = 16\n",
    "embed_dim = 8\n",
    "batch_size, seq_len = 2, 10\n",
    "\n",
    "ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "tok_emb = TokenEmbedding(vocab_size, embed_dim)\n",
    "pos_emb = PositionalEmbedding(max_seq_len, embed_dim)\n",
    "\n",
    "t = tok_emb(ids)\n",
    "p = pos_emb(ids)\n",
    "s = t + p\n",
    "\n",
    "print(\"Token emb shape:\", t.shape)\n",
    "print(\"Pos emb shape:\", p.shape)\n",
    "print(\"Sum shape:\", s.shape)\n",
    "print(\"\\nExample token embedding[0,0]:\", t[0, 0])\n",
    "print(\"Example pos embedding[0,0]:\", p[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc33562f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 5, 8])\n",
      "FFN output shape: torch.Size([2, 5, 8])\n",
      "LayerNorm output shape: torch.Size([2, 5, 8])\n",
      "\n",
      "Mean over last dim before LN (first token): -0.17933684587478638\n",
      "Std over last dim before LN (first token): 0.2778705060482025\n",
      "\n",
      "Mean over last dim after LN (first token): 1.4901161193847656e-08\n",
      "Std over last dim after LN (first token): 0.9999353289604187\n"
     ]
    }
   ],
   "source": [
    "batch_size, seq_len, d_model = 2, 5, 8\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "ff = FeedForward(d_model)\n",
    "ln = LayerNorm(d_model)\n",
    "\n",
    "y = ff(x)\n",
    "z = ln(y)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"FFN output shape:\", y.shape)\n",
    "print(\"LayerNorm output shape:\", z.shape)\n",
    "\n",
    "# Opcional: ver medias y desviaciones por posición\n",
    "print(\"\\nMean over last dim before LN (first token):\", y[0, 0].mean().item())\n",
    "print(\"Std over last dim before LN (first token):\", y[0, 0].std(unbiased=False).item())\n",
    "\n",
    "print(\"\\nMean over last dim after LN (first token):\", z[0, 0].mean().item())\n",
    "print(\"Std over last dim after LN (first token):\", z[0, 0].std(unbiased=False).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52fdc1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final output shape: torch.Size([2, 10, 8])\n",
      "Attention weights shape: torch.Size([2, 2, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "# Mini pipeline: ids -> embeddings -> MHA -> FFN + LN\n",
    "\n",
    "vocab_size = 50\n",
    "max_seq_len = 16\n",
    "embed_dim = 8\n",
    "num_heads = 2\n",
    "batch_size, seq_len = 2, 10\n",
    "\n",
    "ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "tok_emb = TokenEmbedding(vocab_size, embed_dim)\n",
    "pos_emb = PositionalEmbedding(max_seq_len, embed_dim)\n",
    "mha = MultiHeadAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "ff = FeedForward(embed_dim)\n",
    "ln1 = LayerNorm(embed_dim)\n",
    "ln2 = LayerNorm(embed_dim)\n",
    "\n",
    "x = tok_emb(ids) + pos_emb(ids)\n",
    "\n",
    "mask = create_causal_mask(seq_len, device=x.device)\n",
    "\n",
    "att_out, att_weights = mha(x, mask=mask)\n",
    "x = x + att_out            # residual 1\n",
    "x = ln1(x)\n",
    "\n",
    "ff_out = ff(x)\n",
    "x = x + ff_out             # residual 2\n",
    "x = ln2(x)\n",
    "\n",
    "print(\"Final output shape:\", x.shape)\n",
    "print(\"Attention weights shape:\", att_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3a121e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenizer sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a762ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers in /opt/homebrew/lib/python3.10/site-packages (0.22.1)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /opt/homebrew/lib/python3.10/site-packages (from tokenizers) (1.2.3)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (3.20.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (2025.12.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (0.28.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/sultan/Library/Python/3.10/lib/python/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (6.0.3)\n",
      "Requirement already satisfied: shellingham in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (1.5.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (4.67.1)\n",
      "Requirement already satisfied: typer-slim in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (0.21.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/sultan/Library/Python/3.10/lib/python/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (4.15.0)\n",
      "Requirement already satisfied: anyio in /opt/homebrew/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers) (4.12.0)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/homebrew/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/homebrew/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers) (0.16.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/sultan/Library/Python/3.10/lib/python/site-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers) (1.3.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/homebrew/lib/python3.10/site-packages (from typer-slim->huggingface-hub<2.0,>=0.16.4->tokenizers) (8.3.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.10/bin/python3.10 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf998838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/sultan/DataScience/LLM-From-Scratch-Project/V2/notebooks'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0a52a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec890a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tokenizers.Tokenizer, 'Tokenizer')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "TOKENIZER_PATH = \"../models/tokenizers/oscar_bpe_v2/tokenizer.json\"\n",
    "tokenizer = Tokenizer.from_file(TOKENIZER_PATH)\n",
    "\n",
    "type(tokenizer), tokenizer.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3154dbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "TOKENIZER_PATH = \"../models/tokenizers/oscar_bpe_v2/tokenizer.json\"\n",
    "print(\"Exists:\", os.path.exists(TOKENIZER_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6170389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tokenizers.Tokenizer, 'Tokenizer')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer), tokenizer.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "952bcd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_tokens: 29\n",
      "first_token_ids: [3669, 374, 299, 272, 289, 805, 767, 36, 655, 314, 70, 274, 272, 289, 2634, 767, 19, 425, 332, 3634, 272, 275, 461, 245, 1040, 1551, 321, 1468, 19]\n",
      "first_tokens: ['Un', 'Ġper', 'ro', 'Ġes', 'Ġun', 'Ġcan', 'ino', '?', 'ĠUn', 'Ġg', 'a', 'to', 'Ġes', 'Ġun', 'Ġfel', 'ino', '.', 'ĠLa', 'ĠT', 'ierra', 'Ġes', 'Ġel', 'Ġ3', 'er', 'Ġplan', 'eta', 'Ġdel', 'ĠSol', '.']\n",
      "decoded: Un perro es un canino? Un gato es un felino. La Tierra es el 3er planeta del Sol.\n"
     ]
    }
   ],
   "source": [
    "text = \"Un perro es un canino? Un gato es un felino. La Tierra es el 3er planeta del Sol.\"\n",
    "\n",
    "enc = tokenizer.encode(text)\n",
    "\n",
    "print(\"n_tokens:\", len(enc.ids))\n",
    "print(\"first_token_ids:\", enc.ids[:30])\n",
    "print(\"first_tokens:\", enc.tokens[:30])\n",
    "\n",
    "decoded = tokenizer.decode(enc.ids)\n",
    "print(\"decoded:\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d75bff42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook cwd: /Users/sultan/DataScience/LLM-From-Scratch-Project/V2/notebooks\n",
      "Search root : /Users/sultan/DataScience\n",
      "\n",
      "Found 151 candidate files\n",
      "\n",
      "   13.44 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/Base/models/checkpoints_oscar_long/gpt_char_best.pt\n",
      "   13.44 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/V2/models/checkpoints_oscar_long/gpt_char_best.pt\n",
      "   13.44 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/V1/models/checkpoints_oscar_long/gpt_char_best.pt\n",
      "   13.44 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/V2/models/checkpoints_oscar_debug/gpt_char_best.pt\n",
      "   13.44 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/V1/models/checkpoints_oscar_debug/gpt_char_best.pt\n",
      "   13.44 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/Base/models/checkpoints_oscar_debug/gpt_char_best.pt\n",
      "    5.99 MB  -  /Users/sultan/DataScience/RAG/Test RAG 2024/RAG/Tutorial/chroma_persistent_storage/34ef67d4-af93-4251-ba55-1df699839d5f/data_level0.bin\n",
      "    4.54 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/Base/models/checkpoints_oscar_long/gpt_char_cls_toy.pt\n",
      "    4.54 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/V2/models/checkpoints_oscar_long/gpt_char_cls_toy.pt\n",
      "    4.54 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/V1/models/checkpoints_oscar_long/gpt_char_cls_toy.pt\n",
      "    4.52 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/Base/models/checkpoints_oscar_long/gpt_char_cls_toy_v2.pt\n",
      "    4.52 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/V2/models/checkpoints_oscar_long/gpt_char_cls_toy_v2.pt\n",
      "    4.52 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/V1/models/checkpoints_oscar_long/gpt_char_cls_toy_v2.pt\n",
      "    4.52 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/Base/models/checkpoints_oscar_long/gpt_char_instructions.pt\n",
      "    4.52 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/V2/models/checkpoints_oscar_long/gpt_char_instructions.pt\n",
      "    4.52 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/V1/models/checkpoints_oscar_long/gpt_char_instructions.pt\n",
      "    3.06 MB  -  /Users/sultan/DataScience/RAG/RAG-Streamlit/chroma_db/8090dd62-8f90-467a-b5d1-f8b3a5cb1d9d/data_level0.bin\n",
      "    3.06 MB  -  /Users/sultan/DataScience/RAG/RAG_FROM_SCRATCH/chroma_db/8090dd62-8f90-467a-b5d1-f8b3a5cb1d9d/data_level0.bin\n",
      "    3.06 MB  -  /Users/sultan/DataScience/RAG/Test RAG 2024/RAG/TutPaulo/chroma_db/e161f161-6f29-4f75-b1cc-7aaae77d74ca/data_level0.bin\n",
      "    0.35 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/V1/models/checkpoints_tiny/gpt_char_best.pt\n",
      "    0.35 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/Base/models/checkpoints_tiny/gpt_char_best.pt\n",
      "    0.35 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/V2/models/checkpoints_tiny/gpt_char_best.pt\n",
      "    0.35 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/Base/models/checkpoints_test/toy_checkpoint.pt\n",
      "    0.35 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/V1/models/checkpoints_test/toy_checkpoint.pt\n",
      "    0.35 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/V2/models/checkpoints_test/toy_checkpoint.pt\n",
      "    0.07 MB  -  /Users/sultan/DataScience/RAG/RAG-Streamlit/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py\n",
      "    0.07 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/V1/.venv/lib/python3.13/site-packages/torch/utils/checkpoint.py\n",
      "    0.07 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/V2/.venv/lib/python3.13/site-packages/torch/utils/checkpoint.py\n",
      "    0.07 MB  -  /Users/sultan/DataScience/RAG/RAG_FROM_SCRATCH/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py\n",
      "    0.06 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/V2/.venv/lib/python3.13/site-packages/torch/utils/__pycache__/checkpoint.cpython-313.pyc\n",
      "    0.06 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/V1/.venv/lib/python3.13/site-packages/torch/utils/__pycache__/checkpoint.cpython-313.pyc\n",
      "    0.06 MB  -  /Users/sultan/DataScience/RAG/RAG_FROM_SCRATCH/.venv/lib/python3.12/site-packages/torch/utils/__pycache__/checkpoint.cpython-312.pyc\n",
      "    0.06 MB  -  /Users/sultan/DataScience/RAG/RAG-Streamlit/.venv/lib/python3.12/site-packages/torch/utils/__pycache__/checkpoint.cpython-312.pyc\n",
      "    0.06 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/Base/models/checkpoints_oscar_long/char_tokenizer.pt\n",
      "    0.06 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/V2/models/checkpoints_oscar_long/char_tokenizer.pt\n",
      "    0.06 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/Base/models/checkpoints_oscar_debug/char_tokenizer.pt\n",
      "    0.06 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/V2/models/checkpoints_oscar_debug/char_tokenizer.pt\n",
      "    0.06 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/V1/models/checkpoints_oscar_long/char_tokenizer.pt\n",
      "    0.06 MB  -  /Users/sultan/DataScience/LLM-From-Scratch-Project/V1/models/checkpoints_oscar_debug/char_tokenizer.pt\n",
      "    0.04 MB  -  /Users/sultan/DataScience/RAG/RAG-Streamlit/.venv/lib/python3.12/site-packages/transformers/models/megatron_gpt2/checkpoint_reshaping_and_interoperability.py\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "\n",
    "# subir 2 niveles: V2/notebooks -> V2 -> (repo root suele estar arriba de V2)\n",
    "repo_root = ROOT\n",
    "for _ in range(3):\n",
    "    repo_root = repo_root.parent\n",
    "\n",
    "print(\"Notebook cwd:\", ROOT)\n",
    "print(\"Search root :\", repo_root)\n",
    "\n",
    "patterns = [\"**/*.pt\", \"**/*.pth\", \"**/*.bin\", \"**/checkpoint*\", \"**/ckpt*\"]\n",
    "hits = []\n",
    "for p in patterns:\n",
    "    hits += list(repo_root.glob(p))\n",
    "\n",
    "hits = sorted(set(hits), key=lambda x: x.stat().st_size if x.exists() else 0, reverse=True)\n",
    "\n",
    "print(f\"\\nFound {len(hits)} candidate files\\n\")\n",
    "for h in hits[:40]:\n",
    "    size_mb = h.stat().st_size / (1024**2)\n",
    "    print(f\"{size_mb:8.2f} MB  -  {h}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ead97fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.9.1-cp310-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.10/site-packages (from torch) (3.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/sultan/Library/Python/3.10/lib/python/site-packages (from torch) (4.15.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /opt/homebrew/lib/python3.10/site-packages (from torch) (2025.12.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading markupsafe-3.0.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (2.7 kB)\n",
      "Downloading torch-2.9.1-cp310-none-macosx_11_0_arm64.whl (74.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading markupsafe-3.0.3-cp310-cp310-macosx_11_0_arm64.whl (12 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, MarkupSafe, jinja2, torch\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [torch]32m5/6\u001b[0m [torch]kx]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.3 jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 sympy-1.14.0 torch-2.9.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.10/bin/python3.10 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d7ea0395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "keys: ['model_state_dict', 'optimizer_state_dict', 'epoch', 'global_step', 'val_loss', 'training_config']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "ckpt_path = Path(\"/Users/sultan/DataScience/LLM-From-Scratch-Project/V2/models/checkpoints_oscar_long/gpt_char_best.pt\")\n",
    "\n",
    "ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "print(type(ckpt))\n",
    "\n",
    "# imprime claves (o atributos) para ver qué guarda\n",
    "if isinstance(ckpt, dict):\n",
    "    print(\"keys:\", list(ckpt.keys())[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5c0b203b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict,\n",
       " {'batch_size': 16,\n",
       "  'learning_rate': 0.0003,\n",
       "  'weight_decay': 0.01,\n",
       "  'betas': (0.9, 0.95),\n",
       "  'max_grad_norm': 1.0,\n",
       "  'log_every': 10,\n",
       "  'seed': 42,\n",
       "  'device': 'mps'})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = ckpt[\"training_config\"]\n",
    "type(cfg), cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e5956f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_keys: 69\n",
      "sample keys: ['tok_embedding.embedding.weight', 'pos_embedding.pos_embedding.weight', 'blocks.0.ln1.ln.weight', 'blocks.0.ln1.ln.bias', 'blocks.0.attn.q_proj.weight', 'blocks.0.attn.q_proj.bias', 'blocks.0.attn.k_proj.weight', 'blocks.0.attn.k_proj.bias', 'blocks.0.attn.v_proj.weight', 'blocks.0.attn.v_proj.bias', 'blocks.0.attn.out_proj.weight', 'blocks.0.attn.out_proj.bias', 'blocks.0.ln2.ln.weight', 'blocks.0.ln2.ln.bias', 'blocks.0.ff.fc1.weight', 'blocks.0.ff.fc1.bias', 'blocks.0.ff.fc2.weight', 'blocks.0.ff.fc2.bias', 'blocks.1.ln1.ln.weight', 'blocks.1.ln1.ln.bias', 'blocks.1.attn.q_proj.weight', 'blocks.1.attn.q_proj.bias', 'blocks.1.attn.k_proj.weight', 'blocks.1.attn.k_proj.bias', 'blocks.1.attn.v_proj.weight', 'blocks.1.attn.v_proj.bias', 'blocks.1.attn.out_proj.weight', 'blocks.1.attn.out_proj.bias', 'blocks.1.ln2.ln.weight', 'blocks.1.ln2.ln.bias']\n",
      "candidate: tok_embedding.embedding.weight torch.Size([2796, 128])\n",
      "candidate: pos_embedding.pos_embedding.weight torch.Size([128, 128])\n"
     ]
    }
   ],
   "source": [
    "msd = ckpt[\"model_state_dict\"]\n",
    "keys = list(msd.keys())\n",
    "print(\"n_keys:\", len(keys))\n",
    "print(\"sample keys:\", keys[:30])\n",
    "\n",
    "# intenta detectar la matriz de embeddings (casi siempre existe)\n",
    "for k in keys:\n",
    "    if \"embed\" in k.lower() or \"tok_emb\" in k.lower() or \"token\" in k.lower():\n",
    "        print(\"candidate:\", k, msd[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5676c1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer vocab size: 4096\n"
     ]
    }
   ],
   "source": [
    "print(\"tokenizer vocab size:\", tokenizer.get_vocab_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b0f2a1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_ckpt: 2796\n"
     ]
    }
   ],
   "source": [
    "vocab_ckpt = ckpt[\"model_state_dict\"][\"tok_embedding.embedding.weight\"].shape[0]\n",
    "print(\"vocab_ckpt:\", vocab_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e44a476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found tokenizer.json: 1\n",
      "/Users/sultan/DataScience/LLM-From-Scratch-Project/V2/models/tokenizers/oscar_bpe_v2/tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(\"/Users/sultan/DataScience/LLM-From-Scratch-Project\")\n",
    "tok_jsons = sorted(ROOT.glob(\"**/tokenizer.json\"))\n",
    "print(\"found tokenizer.json:\", len(tok_jsons))\n",
    "for p in tok_jsons[:40]:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d024f999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok2 vocab size: 4096\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "tok_path = \"/Users/sultan/DataScience/LLM-From-Scratch-Project/V2/models/tokenizers/oscar_bpe_v2/tokenizer.json\"\n",
    "tok2 = Tokenizer.from_file(tok_path)\n",
    "print(\"tok2 vocab size:\", tok2.get_vocab_size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4e12c805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found checkpoints with detectable token embedding: 15\n",
      "vocab=2796  key=tok_embedding.embedding.weight       /Users/sultan/DataScience/LLM-From-Scratch-Project/V1/models/checkpoints_oscar_long/gpt_char_best.pt\n",
      "vocab=2796  key=tok_embedding.embedding.weight       /Users/sultan/DataScience/LLM-From-Scratch-Project/V2/models/checkpoints_oscar_long/gpt_char_best.pt\n",
      "vocab=2796  key=tok_embedding.embedding.weight       /Users/sultan/DataScience/LLM-From-Scratch-Project/Base/models/checkpoints_oscar_long/gpt_char_best.pt\n",
      "vocab=2796  key=tok_embedding.embedding.weight       /Users/sultan/DataScience/LLM-From-Scratch-Project/V1/models/checkpoints_oscar_debug/gpt_char_best.pt\n",
      "vocab=2796  key=tok_embedding.embedding.weight       /Users/sultan/DataScience/LLM-From-Scratch-Project/V2/models/checkpoints_oscar_debug/gpt_char_best.pt\n",
      "vocab=2796  key=tok_embedding.embedding.weight       /Users/sultan/DataScience/LLM-From-Scratch-Project/Base/models/checkpoints_oscar_debug/gpt_char_best.pt\n",
      "vocab=2796  key=tok_embedding.embedding.weight       /Users/sultan/DataScience/LLM-From-Scratch-Project/V1/models/checkpoints_oscar_long/gpt_char_instructions.pt\n",
      "vocab=2796  key=tok_embedding.embedding.weight       /Users/sultan/DataScience/LLM-From-Scratch-Project/V2/models/checkpoints_oscar_long/gpt_char_instructions.pt\n",
      "vocab=2796  key=tok_embedding.embedding.weight       /Users/sultan/DataScience/LLM-From-Scratch-Project/Base/models/checkpoints_oscar_long/gpt_char_instructions.pt\n",
      "vocab=37    key=tok_embedding.embedding.weight       /Users/sultan/DataScience/LLM-From-Scratch-Project/V1/models/checkpoints_tiny/gpt_char_best.pt\n",
      "vocab=37    key=tok_embedding.embedding.weight       /Users/sultan/DataScience/LLM-From-Scratch-Project/V2/models/checkpoints_tiny/gpt_char_best.pt\n",
      "vocab=37    key=tok_embedding.embedding.weight       /Users/sultan/DataScience/LLM-From-Scratch-Project/Base/models/checkpoints_tiny/gpt_char_best.pt\n",
      "vocab=40    key=tok_embedding.embedding.weight       /Users/sultan/DataScience/LLM-From-Scratch-Project/V1/models/checkpoints_test/toy_checkpoint.pt\n",
      "vocab=40    key=tok_embedding.embedding.weight       /Users/sultan/DataScience/LLM-From-Scratch-Project/V2/models/checkpoints_test/toy_checkpoint.pt\n",
      "vocab=40    key=tok_embedding.embedding.weight       /Users/sultan/DataScience/LLM-From-Scratch-Project/Base/models/checkpoints_test/toy_checkpoint.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(\"/Users/sultan/DataScience/LLM-From-Scratch-Project\")\n",
    "\n",
    "ckpt_paths = sorted(\n",
    "    list(ROOT.glob(\"**/*.pt\")) + list(ROOT.glob(\"**/*.pth\")),\n",
    "    key=lambda p: p.stat().st_size,\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "def get_vocab_from_ckpt(path):\n",
    "    try:\n",
    "        ckpt = torch.load(path, map_location=\"cpu\")\n",
    "        sd = ckpt.get(\"model_state_dict\", ckpt)  # soporta checkpoints dict o state_dict directo\n",
    "        # buscamos el embedding típico\n",
    "        for k in [\"tok_embedding.embedding.weight\", \"token_embedding.weight\", \"tok_embedding.weight\", \"wte.weight\"]:\n",
    "            if k in sd:\n",
    "                return sd[k].shape[0], k\n",
    "        return None, None\n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "hits = []\n",
    "for p in ckpt_paths:\n",
    "    vocab, key = get_vocab_from_ckpt(p)\n",
    "    if vocab is not None:\n",
    "        hits.append((vocab, str(p), key))\n",
    "\n",
    "print(\"Found checkpoints with detectable token embedding:\", len(hits))\n",
    "for vocab, p, key in hits[:40]:\n",
    "    print(f\"vocab={vocab:<5} key={key:<35}  {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab4c5551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "keys: ['stoi', 'itos']\n",
      "stoi len= 2796\n",
      "itos len= 2796\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "tok_path = Path(\"/Users/sultan/DataScience/LLM-From-Scratch-Project/V2/models/checkpoints_oscar_long/char_tokenizer.pt\")\n",
    "tok = torch.load(tok_path, map_location=\"cpu\")\n",
    "\n",
    "print(type(tok))\n",
    "if isinstance(tok, dict):\n",
    "    print(\"keys:\", list(tok.keys())[:30])\n",
    "    for k,v in tok.items():\n",
    "        if hasattr(v, \"__len__\"):\n",
    "            print(k, \"len=\", len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c27afcd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_tokens: 81\n",
      "first_ids: [71, 96, 18, 98, 87, 100, 100, 97, 18, 87, 101, 18, 103, 96, 18, 85, 83, 96, 91, 96, 97, 49, 18, 71, 96, 18, 89, 83, 102, 97, 18, 87, 101, 18, 103, 96, 18, 88, 87, 94]\n",
      "decoded_ok: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Cargar tokenizer dict\n",
    "tok_path = Path(\"/Users/sultan/DataScience/LLM-From-Scratch-Project/V2/models/checkpoints_oscar_long/char_tokenizer.pt\")\n",
    "tok = torch.load(tok_path, map_location=\"cpu\")\n",
    "stoi, itos = tok[\"stoi\"], tok[\"itos\"]\n",
    "\n",
    "unk_id = stoi.get(\"<unk>\", None)  # si no existe, luego te digo qué hacemos\n",
    "\n",
    "def encode(text: str):\n",
    "    ids = []\n",
    "    for ch in text:\n",
    "        if ch in stoi:\n",
    "            ids.append(stoi[ch])\n",
    "        else:\n",
    "            if unk_id is None:\n",
    "                raise ValueError(f\"Character not in vocab and no <unk> token found: {repr(ch)}\")\n",
    "            ids.append(unk_id)\n",
    "    return ids\n",
    "\n",
    "def decode(ids):\n",
    "    return \"\".join(itos[i] for i in ids)\n",
    "\n",
    "# 2) Prueba\n",
    "text = \"Un perro es un canino? Un gato es un felino. La Tierra es el 3er planeta del Sol.\"\n",
    "ids = encode(text)\n",
    "\n",
    "print(\"n_tokens:\", len(ids))\n",
    "print(\"first_ids:\", ids[:40])\n",
    "print(\"decoded_ok:\", decode(ids) == text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc76f4e",
   "metadata": {},
   "source": [
    "# Me parece que desde acá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "621deb6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: /Users/sultan/DataScience/LLM-From-Scratch-Project\n",
      "CWD: /Users/sultan/DataScience/LLM-From-Scratch-Project/V2/notebooks\n"
     ]
    }
   ],
   "source": [
    "# 1) Crear una sección “SANITY CHECKS (no training)” y reiniciar variables\n",
    "\n",
    "# --- SANITY CHECKS (NO TRAINING) ---\n",
    "# This notebook only reads artifacts and prints diagnostics.\n",
    "\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(\"/Users/sultan/DataScience/LLM-From-Scratch-Project\")\n",
    "V2_ROOT = PROJECT_ROOT / \"V2\"\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"CWD:\", Path.cwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6ef2fe40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CKPT_PATH: /Users/sultan/DataScience/LLM-From-Scratch-Project/V2/models/checkpoints_oscar_long/gpt_char_best.pt\n",
      "exists: True\n",
      "size_mb: 13.439669609069824\n",
      "ckpt keys: ['model_state_dict', 'optimizer_state_dict', 'epoch', 'global_step', 'val_loss', 'training_config']\n",
      "has model_state_dict: True\n"
     ]
    }
   ],
   "source": [
    "# 2) Verificar checkpoint elegido (existencia + tamaño + keys)\n",
    "\n",
    "import torch\n",
    "\n",
    "CKPT_PATH = V2_ROOT / \"models/checkpoints_oscar_long/gpt_char_best.pt\"\n",
    "print(\"CKPT_PATH:\", CKPT_PATH)\n",
    "print(\"exists:\", CKPT_PATH.exists())\n",
    "print(\"size_mb:\", CKPT_PATH.stat().st_size / (1024**2))\n",
    "\n",
    "ckpt = torch.load(CKPT_PATH, map_location=\"cpu\")\n",
    "print(\"ckpt keys:\", list(ckpt.keys()))\n",
    "print(\"has model_state_dict:\", \"model_state_dict\" in ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d0e0ca6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char_tokenizer path: /Users/sultan/DataScience/LLM-From-Scratch-Project/V2/models/checkpoints_oscar_long/char_tokenizer.pt exists: True\n",
      "vocab size: 2796 2796\n",
      "n_tokens: 44\n",
      "first_ids: [71, 96, 18, 98, 87, 100, 100, 97, 18, 87, 101, 18, 103, 96, 18, 85, 83, 96, 91, 96, 97, 49, 18, 71, 96, 18, 89, 83, 102, 97, 18, 87, 101, 18, 103, 96, 18, 88, 87, 94]\n",
      "decoded_ok: True\n"
     ]
    }
   ],
   "source": [
    "# 3) Cargar tokenizer del checkpoint (stoi/itos) y re-test rápido de encode/decode\n",
    "\n",
    "tok = ckpt.get(\"training_config\", None)  # solo para no confundir nombres\n",
    "char_tok_path = V2_ROOT / \"models/checkpoints_oscar_long/char_tokenizer.pt\"\n",
    "print(\"char_tokenizer path:\", char_tok_path, \"exists:\", char_tok_path.exists())\n",
    "\n",
    "char_tok = torch.load(char_tok_path, map_location=\"cpu\")  # {'stoi':..., 'itos':...}\n",
    "stoi, itos = char_tok[\"stoi\"], char_tok[\"itos\"]\n",
    "print(\"vocab size:\", len(stoi), len(itos))\n",
    "\n",
    "def encode_chars(s: str):\n",
    "    return [stoi.get(ch, stoi.get(\"<unk>\", 0)) for ch in s]\n",
    "\n",
    "def decode_chars(ids):\n",
    "    return \"\".join([itos[i] for i in ids])\n",
    "\n",
    "text = \"Un perro es un canino? Un gato es un felino.\"\n",
    "ids = encode_chars(text)\n",
    "back = decode_chars(ids)\n",
    "\n",
    "print(\"n_tokens:\", len(ids))\n",
    "print(\"first_ids:\", ids[:40])\n",
    "print(\"decoded_ok:\", back == text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2f4c6dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "py_files: 8891\n",
      "model candidates: 288\n",
      "/Users/sultan/DataScience/LLM-From-Scratch-Project/V2/.venv/lib/python3.13/site-packages/aiohttp/web_request.py\n",
      "/Users/sultan/DataScience/LLM-From-Scratch-Project/V2/.venv/lib/python3.13/site-packages/huggingface_hub/hub_mixin.py\n",
      "/Users/sultan/DataScience/LLM-From-Scratch-Project/V2/.venv/lib/python3.13/site-packages/huggingface_hub/hf_api.py\n",
      "/Users/sultan/DataScience/LLM-From-Scratch-Project/V2/.venv/lib/python3.13/site-packages/streamlit/testing/v1/local_script_runner.py\n",
      "/Users/sultan/DataScience/LLM-From-Scratch-Project/V2/.venv/lib/python3.13/site-packages/sympy/testing/runtests.py\n",
      "/Users/sultan/DataScience/LLM-From-Scratch-Project/V2/.venv/lib/python3.13/site-packages/sympy/physics/control/lti.py\n",
      "/Users/sultan/DataScience/LLM-From-Scratch-Project/V2/.venv/lib/python3.13/site-packages/sympy/parsing/ast_parser.py\n",
      "/Users/sultan/DataScience/LLM-From-Scratch-Project/V2/.venv/lib/python3.13/site-packages/sympy/parsing/sympy_parser.py\n",
      "/Users/sultan/DataScience/LLM-From-Scratch-Project/V2/.venv/lib/python3.13/site-packages/sympy/parsing/latex/lark/transformer.py\n",
      "/Users/sultan/DataScience/LLM-From-Scratch-Project/V2/.venv/lib/python3.13/site-packages/sympy/parsing/latex/lark/latex_parser.py\n",
      "/Users/sultan/DataScience/LLM-From-Scratch-Project/V2/.venv/lib/python3.13/site-packages/sympy/parsing/tests/test_custom_latex.py\n",
      "/Users/sultan/DataScience/LLM-From-Scratch-Project/V2/.venv/lib/python3.13/site-packages/jinja2/visitor.py\n",
      "/Users/sultan/DataScience/LLM-From-Scratch-Project/V2/.venv/lib/python3.13/site-packages/jinja2/optimizer.py\n",
      "/Users/sultan/DataScience/LLM-From-Scratch-Project/V2/.venv/lib/python3.13/site-packages/altair/utils/data.py\n",
      "/Users/sultan/DataScience/LLM-From-Scratch-Project/V2/.venv/lib/python3.13/site-packages/altair/vegalite/data.py\n",
      "/Users/sultan/DataScience/LLM-From-Scratch-Project/V2/.venv/lib/python3.13/site-packages/click/core.py\n",
      "/Users/sultan/DataScience/LLM-From-Scratch-Project/V2/.venv/lib/python3.13/site-packages/torch/_jit_internal.py\n",
      "/Users/sultan/DataScience/LLM-From-Scratch-Project/V2/.venv/lib/python3.13/site-packages/torch/_lobpcg.py\n",
      "/Users/sultan/DataScience/LLM-From-Scratch-Project/V2/.venv/lib/python3.13/site-packages/torch/_higher_order_ops/invoke_subgraph.py\n",
      "/Users/sultan/DataScience/LLM-From-Scratch-Project/V2/.venv/lib/python3.13/site-packages/torch/_higher_order_ops/while_loop.py\n",
      "/Users/sultan/DataScience/LLM-From-Scratch-Project/V2/.venv/lib/python3.13/site-packages/torch/_higher_order_ops/base_hop.py\n",
      "/Users/sultan/DataScience/LLM-From-Scratch-Project/V2/.venv/lib/python3.13/site-packages/torch/_higher_order_ops/cond.py\n",
      "/Users/sultan/DataScience/LLM-From-Scratch-Project/V2/.venv/lib/python3.13/site-packages/torch/_higher_order_ops/map.py\n",
      "/Users/sultan/DataScience/LLM-From-Scratch-Project/V2/.venv/lib/python3.13/site-packages/torch/_higher_order_ops/scan.py\n",
      "/Users/sultan/DataScience/LLM-From-Scratch-Project/V2/.venv/lib/python3.13/site-packages/torch/_higher_order_ops/associative_scan.py\n",
      "/Users/sultan/DataScience/LLM-From-Scratch-Project/V2/.venv/lib/python3.13/site-packages/torch/_higher_order_ops/flex_attention.py\n",
      "/Users/sultan/DataScience/LLM-From-Scratch-Project/V2/.venv/lib/python3.13/site-packages/torch/_higher_order_ops/schema.py\n",
      "/Users/sultan/DataScience/LLM-From-Scratch-Project/V2/.venv/lib/python3.13/site-packages/torch/_functorch/autograd_function.py\n",
      "/Users/sultan/DataScience/LLM-From-Scratch-Project/V2/.venv/lib/python3.13/site-packages/torch/_functorch/aot_autograd.py\n",
      "/Users/sultan/DataScience/LLM-From-Scratch-Project/V2/.venv/lib/python3.13/site-packages/torch/_functorch/eager_transforms.py\n"
     ]
    }
   ],
   "source": [
    "# 4) Encontrar la clase del modelo para poder generar next-token\n",
    "\n",
    "import re\n",
    "\n",
    "py_files = list(V2_ROOT.rglob(\"*.py\"))\n",
    "print(\"py_files:\", len(py_files))\n",
    "\n",
    "candidates = []\n",
    "for p in py_files:\n",
    "    txt = p.read_text(errors=\"ignore\")\n",
    "    if re.search(r\"class\\s+.*GPT|class\\s+.*Transformer|def\\s+forward\", txt):\n",
    "        candidates.append(p)\n",
    "\n",
    "print(\"model candidates:\", len(candidates))\n",
    "for p in candidates[:30]:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf960ed",
   "metadata": {},
   "source": [
    "# De nuevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8cf92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1) Re-hacer el “model candidates” EXCLUYENDO .venv y site-packages\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "V2_ROOT = Path(\"/Users/sultan/DataScience/LLM-From-Scratch-Project/V2\")\n",
    "\n",
    "py_files = [p for p in V2_ROOT.rglob(\"*.py\") if \".venv\" not in str(p) and \"site-packages\" not in str(p)]\n",
    "print(\"py_files (filtered):\", len(py_files))\n",
    "\n",
    "candidates = []\n",
    "for p in py_files:\n",
    "    txt = p.read_text(errors=\"ignore\")\n",
    "    if re.search(r\"class\\s+.*GPT|class\\s+.*Transformer|def\\s+forward\", txt):\n",
    "        candidates.append(p)\n",
    "\n",
    "print(\"model candidates (filtered):\", len(candidates))\n",
    "for p in candidates[:40]:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fb13a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 2) Buscar “huellas” exactas de tu checkpoint (tok_embedding / pos_embedding / blocks.)\n",
    "\n",
    "needles = [\n",
    "    \"tok_embedding\",\n",
    "    \"pos_embedding\",\n",
    "    \"q_proj\",\n",
    "    \"k_proj\",\n",
    "    \"v_proj\",\n",
    "    \"out_proj\",\n",
    "    \"blocks.\"\n",
    "]\n",
    "\n",
    "hits = []\n",
    "for p in py_files:\n",
    "    txt = p.read_text(errors=\"ignore\")\n",
    "    score = sum(n in txt for n in needles)\n",
    "    if score >= 2:  # ajustable\n",
    "        hits.append((score, p))\n",
    "\n",
    "hits.sort(reverse=True, key=lambda x: x[0])\n",
    "\n",
    "print(\"strong hits:\", len(hits))\n",
    "for score, p in hits[:30]:\n",
    "    print(score, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e593932b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 3) Inferir la “config” del modelo desde el checkpoint (para instanciar correcto)\n",
    "\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "CKPT_PATH = \"/Users/sultan/DataScience/LLM-From-Scratch-Project/V2/models/checkpoints_oscar_long/gpt_char_best.pt\"\n",
    "ckpt = torch.load(CKPT_PATH, map_location=\"cpu\")\n",
    "sd = ckpt[\"model_state_dict\"]\n",
    "\n",
    "# vocab y d_model\n",
    "vocab_size, d_model = sd[\"tok_embedding.embedding.weight\"].shape\n",
    "\n",
    "# max_seq_len\n",
    "max_seq_len = sd[\"pos_embedding.pos_embedding.weight\"].shape[0]\n",
    "\n",
    "# n_layers\n",
    "layer_ids = set()\n",
    "for k in sd.keys():\n",
    "    if k.startswith(\"blocks.\"):\n",
    "        layer_ids.add(int(k.split(\".\")[1]))\n",
    "n_layers = max(layer_ids) + 1 if layer_ids else 0\n",
    "\n",
    "print(\"INFERRED:\")\n",
    "print(\"vocab_size:\", vocab_size)\n",
    "print(\"d_model:\", d_model)\n",
    "print(\"max_seq_len:\", max_seq_len)\n",
    "print(\"n_layers:\", n_layers)\n",
    "\n",
    "# si quieres ver una pista de heads/d_ff:\n",
    "for name in [\"blocks.0.attn.q_proj.weight\", \"blocks.0.ff.fc1.weight\"]:\n",
    "    if name in sd:\n",
    "        print(name, sd[name].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787fe82e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
