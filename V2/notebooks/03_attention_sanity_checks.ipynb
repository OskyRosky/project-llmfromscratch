{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "704d8576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python exe: /Users/sultan/DataScience/LLM-From-Scratch-Project/.venv/bin/python\n",
      "Torch version: 2.9.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Añadimos la raíz del proyecto al path de Python\n",
    "PROJECT_ROOT = \"/Users/sultan/DataScience/LLM-From-Scratch-Project\"\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "import torch\n",
    "\n",
    "from src.model.attention import (\n",
    "    create_causal_mask,\n",
    "    scaled_dot_product_attention,\n",
    "    MultiHeadAttention,\n",
    ")\n",
    "\n",
    "from src.model.layers import (\n",
    "    TokenEmbedding,\n",
    "    PositionalEmbedding,\n",
    "    FeedForward,\n",
    "    LayerNorm,\n",
    ")\n",
    "\n",
    "print(\"Python exe:\", sys.executable)\n",
    "print(\"Torch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1f474d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q shape: torch.Size([1, 1, 4, 2])\n",
      "k shape: torch.Size([1, 1, 4, 2])\n",
      "v shape: torch.Size([1, 1, 4, 2])\n",
      "out shape: torch.Size([1, 1, 4, 2])\n",
      "attn shape: torch.Size([1, 1, 4, 4])\n",
      "\n",
      "Causal mask (0 = futuro bloqueado):\n",
      "tensor([[1, 0, 0, 0],\n",
      "        [1, 1, 0, 0],\n",
      "        [1, 1, 1, 0],\n",
      "        [1, 1, 1, 1]], dtype=torch.int32)\n",
      "\n",
      "Attention matrix (head 0):\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5846, 0.4154, 0.0000, 0.0000],\n",
      "        [0.2029, 0.6149, 0.1822, 0.0000],\n",
      "        [0.1886, 0.0650, 0.1352, 0.6112]])\n"
     ]
    }
   ],
   "source": [
    "batch_size, num_heads, seq_len, head_dim = 1, 1, 4, 2\n",
    "\n",
    "q = torch.randn(batch_size, num_heads, seq_len, head_dim)\n",
    "k = torch.randn(batch_size, num_heads, seq_len, head_dim)\n",
    "v = torch.randn(batch_size, num_heads, seq_len, head_dim)\n",
    "\n",
    "mask = create_causal_mask(seq_len, device=q.device)\n",
    "\n",
    "out, attn = scaled_dot_product_attention(q, k, v, mask=mask)\n",
    "\n",
    "print(\"q shape:\", q.shape)\n",
    "print(\"k shape:\", k.shape)\n",
    "print(\"v shape:\", v.shape)\n",
    "print(\"out shape:\", out.shape)\n",
    "print(\"attn shape:\", attn.shape)\n",
    "print(\"\\nCausal mask (0 = futuro bloqueado):\")\n",
    "print(mask[0, 0].int())\n",
    "print(\"\\nAttention matrix (head 0):\")\n",
    "print(attn[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "322d6369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 5, 8])\n",
      "Output shape: torch.Size([2, 5, 8])\n",
      "Attention shape: torch.Size([2, 2, 5, 5])\n",
      "\n",
      "Attention matrix (batch 0, head 0):\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3857, 0.6143, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2425, 0.3793, 0.3782, 0.0000, 0.0000],\n",
      "        [0.2708, 0.1764, 0.2611, 0.2917, 0.0000],\n",
      "        [0.2018, 0.2011, 0.1663, 0.2144, 0.2164]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size, seq_len, embed_dim, num_heads = 2, 5, 8, 2\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, embed_dim)\n",
    "mha = MultiHeadAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "\n",
    "mask = create_causal_mask(seq_len, device=x.device)\n",
    "\n",
    "out, attn = mha(x, mask=mask)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", out.shape)\n",
    "print(\"Attention shape:\", attn.shape)\n",
    "print(\"\\nAttention matrix (batch 0, head 0):\")\n",
    "print(attn[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3f119f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token emb shape: torch.Size([2, 10, 8])\n",
      "Pos emb shape: torch.Size([2, 10, 8])\n",
      "Sum shape: torch.Size([2, 10, 8])\n",
      "\n",
      "Example token embedding[0,0]: tensor([ 1.3288, -2.4420,  1.1842, -0.8649, -6.8487,  0.9799,  0.6968, -0.2026],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Example pos embedding[0,0]: tensor([ 1.3486, -2.1934,  0.7030,  0.8502, -0.6056, -0.5264, -0.4830, -1.2382],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50\n",
    "max_seq_len = 16\n",
    "embed_dim = 8\n",
    "batch_size, seq_len = 2, 10\n",
    "\n",
    "ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "tok_emb = TokenEmbedding(vocab_size, embed_dim)\n",
    "pos_emb = PositionalEmbedding(max_seq_len, embed_dim)\n",
    "\n",
    "t = tok_emb(ids)\n",
    "p = pos_emb(ids)\n",
    "s = t + p\n",
    "\n",
    "print(\"Token emb shape:\", t.shape)\n",
    "print(\"Pos emb shape:\", p.shape)\n",
    "print(\"Sum shape:\", s.shape)\n",
    "print(\"\\nExample token embedding[0,0]:\", t[0, 0])\n",
    "print(\"Example pos embedding[0,0]:\", p[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc33562f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 5, 8])\n",
      "FFN output shape: torch.Size([2, 5, 8])\n",
      "LayerNorm output shape: torch.Size([2, 5, 8])\n",
      "\n",
      "Mean over last dim before LN (first token): -0.17933684587478638\n",
      "Std over last dim before LN (first token): 0.2778705060482025\n",
      "\n",
      "Mean over last dim after LN (first token): 1.4901161193847656e-08\n",
      "Std over last dim after LN (first token): 0.9999353289604187\n"
     ]
    }
   ],
   "source": [
    "batch_size, seq_len, d_model = 2, 5, 8\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "ff = FeedForward(d_model)\n",
    "ln = LayerNorm(d_model)\n",
    "\n",
    "y = ff(x)\n",
    "z = ln(y)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"FFN output shape:\", y.shape)\n",
    "print(\"LayerNorm output shape:\", z.shape)\n",
    "\n",
    "# Opcional: ver medias y desviaciones por posición\n",
    "print(\"\\nMean over last dim before LN (first token):\", y[0, 0].mean().item())\n",
    "print(\"Std over last dim before LN (first token):\", y[0, 0].std(unbiased=False).item())\n",
    "\n",
    "print(\"\\nMean over last dim after LN (first token):\", z[0, 0].mean().item())\n",
    "print(\"Std over last dim after LN (first token):\", z[0, 0].std(unbiased=False).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52fdc1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final output shape: torch.Size([2, 10, 8])\n",
      "Attention weights shape: torch.Size([2, 2, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "# Mini pipeline: ids -> embeddings -> MHA -> FFN + LN\n",
    "\n",
    "vocab_size = 50\n",
    "max_seq_len = 16\n",
    "embed_dim = 8\n",
    "num_heads = 2\n",
    "batch_size, seq_len = 2, 10\n",
    "\n",
    "ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "tok_emb = TokenEmbedding(vocab_size, embed_dim)\n",
    "pos_emb = PositionalEmbedding(max_seq_len, embed_dim)\n",
    "mha = MultiHeadAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "ff = FeedForward(embed_dim)\n",
    "ln1 = LayerNorm(embed_dim)\n",
    "ln2 = LayerNorm(embed_dim)\n",
    "\n",
    "x = tok_emb(ids) + pos_emb(ids)\n",
    "\n",
    "mask = create_causal_mask(seq_len, device=x.device)\n",
    "\n",
    "att_out, att_weights = mha(x, mask=mask)\n",
    "x = x + att_out            # residual 1\n",
    "x = ln1(x)\n",
    "\n",
    "ff_out = ff(x)\n",
    "x = x + ff_out             # residual 2\n",
    "x = ln2(x)\n",
    "\n",
    "print(\"Final output shape:\", x.shape)\n",
    "print(\"Attention weights shape:\", att_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3a121e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenizer sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a762ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers in /opt/homebrew/lib/python3.10/site-packages (0.22.1)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /opt/homebrew/lib/python3.10/site-packages (from tokenizers) (1.2.3)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (3.20.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (2025.12.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (0.28.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/sultan/Library/Python/3.10/lib/python/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (6.0.3)\n",
      "Requirement already satisfied: shellingham in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (1.5.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (4.67.1)\n",
      "Requirement already satisfied: typer-slim in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (0.21.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/sultan/Library/Python/3.10/lib/python/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (4.15.0)\n",
      "Requirement already satisfied: anyio in /opt/homebrew/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers) (4.12.0)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/homebrew/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/homebrew/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers) (0.16.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/sultan/Library/Python/3.10/lib/python/site-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers) (1.3.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/homebrew/lib/python3.10/site-packages (from typer-slim->huggingface-hub<2.0,>=0.16.4->tokenizers) (8.3.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.10/bin/python3.10 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf998838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/sultan/DataScience/LLM-From-Scratch-Project/V2/notebooks'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0a52a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec890a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tokenizers.Tokenizer, 'Tokenizer')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "TOKENIZER_PATH = \"../models/tokenizers/oscar_bpe_v2/tokenizer.json\"\n",
    "tokenizer = Tokenizer.from_file(TOKENIZER_PATH)\n",
    "\n",
    "type(tokenizer), tokenizer.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3154dbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "TOKENIZER_PATH = \"../models/tokenizers/oscar_bpe_v2/tokenizer.json\"\n",
    "print(\"Exists:\", os.path.exists(TOKENIZER_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6170389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tokenizers.Tokenizer, 'Tokenizer')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer), tokenizer.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "952bcd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_tokens: 29\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'tokenizers.Encoding' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m enc \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(text)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_tokens:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(enc))\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst_tokens_ids:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43menc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      8\u001b[0m decoded \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(enc)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoded:\u001b[39m\u001b[38;5;124m\"\u001b[39m, decoded)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tokenizers.Encoding' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "text = \"Un perro es un canino? Un gato es un felino. La Tierra es el 3er planeta del Sol.\"\n",
    "\n",
    "enc = tokenizer.encode(text)\n",
    "\n",
    "print(\"n_tokens:\", len(enc))\n",
    "print(\"first_tokens_ids:\", enc[:30])\n",
    "\n",
    "decoded = tokenizer.decode(enc)\n",
    "print(\"decoded:\", decoded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
