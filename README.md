# LLM From Scratch

 ![class](/ima/ima1.png)

---------------------------------------------

**Repository summary**

1.  **Intro** üß≥

2.  **Tech Stack** ü§ñ


3.  **Features** ü§≥üèΩ


4.  **Process** üë£


5.  **Learning** üí°


6.  **Improvement** üî©


7.  **Running the Project** ‚öôÔ∏è


8 .  **More** üôåüèΩ

For collaboration, discussion, or improvements:

‚Ä¢	GitHub Issues: for bugs or feature requests.

‚Ä¢	Pull Requests: for contributions or new examples.

‚Ä¢	Contact: open an issue or connect via LinkedIn / email (author info in profile).

If this project helps you learn or build better models, consider starring ‚≠ê the repository ‚Äî
it‚Äôs the simplest way to support continued open knowledge sharing.


---------------------------------------------

# :computer: LLM From Scratch  :computer:

---------------------------------------------

# 0. Why build an LLM from Scratch ? 

Building a Large Language Model from scratch is not about competing with industrial-scale systems like GPT-4 or Claude. Instead, it is about understanding‚Äîtruly understanding‚Äîhow modern language models work at the lowest level. When you assemble every component by hand, from tokenization to attention mechanisms to pretraining loops, the architecture stops being a black box and becomes an engineered system whose internal logic you can navigate with confidence.

Most practitioners work with high-level libraries that abstract away the essential mechanics of LLMs. That abstraction is convenient, but it hides the fundamental transformations that give these models their reasoning capabilities: how text becomes numbers, how attention selects and weights context, how residual blocks accumulate knowledge, or how predictions are optimized over billions of tokens. By re-implementing these ideas, you gain a concrete mental model of what is happening at every step of the pipeline.

 ![class](/ima/ima2.png)

This project exists for that purpose. It aims to reconstruct the core pieces of a GPT-style model with clarity and intention, taking nothing for granted. Every function, every shape transformation, and every training step is exposed. You are able to see how a single character or token propagates through embeddings, how attention scores are computed and normalized, how the model predicts the next element in a sequence, and why gradient updates slowly sculpt its internal representation of language.

Once you build an LLM from scratch, you earn three key advantages.
First, you gain the ability to debug and modify any part of the architecture without relying on hidden implementations.
Second, you develop intuition for why certain design decisions‚Äîsuch as positional encodings or normalization layers‚Äîmatter for stability and performance.
And third, you become independent: capable of extending, compressing, adapting or even inventing new architectures instead of depending solely on external frameworks.

This repository is the outcome of that philosophy. It walks through the entire lifecycle of a modern language model‚Äîfrom raw text to a trained model capable of following simple instructions and interacting through a custom interface‚Äîwhile keeping the system transparent and accessible. The result is not a production-ready LLM, but rather a rigorous educational framework: a place where the fundamentals can be learned, inspected, improved and expanded.


# 1. Basic Infraestructure.

Building an LLM from scratch requires a clean, modular, and scalable codebase. The goal is not only to train a model, but to create an environment where experimentation is safe, reproducible, and easy to extend. This project follows a structure inspired by modern ML engineering practices (NLP labs, production AI teams, and open-source frameworks like nanoGPT, PyTorch Lightning, and HuggingFace).

At its core, the infrastructure is designed around four principles:
	1.	Separation of concerns ‚Äî preprocessing logic, model code, training loops, evaluation utilities, and UI demos all live in dedicated modules.
	2.	Reproducibility ‚Äî every step (tokenization, sampling, training configuration) is deterministic and version-controlled.
	3.	Incremental extensibility ‚Äî the project supports a natural evolution:
character-level GPT ‚Üí instruction tuning ‚Üí UI deployment ‚Üí word-level GPT (future V2).
	4.	Ease of experimentation ‚Äî everything can be run via CLI tools or notebooks, without rewriting boilerplate.

The base directory structure is:

```Python
LLM-From-Scratch-Project/
‚îÇ
‚îú‚îÄ‚îÄ data/                 # raw and processed datasets
‚îú‚îÄ‚îÄ models/               # pretrained weights and fine-tuned checkpoints
‚îú‚îÄ‚îÄ src/                  # model architecture, training pipeline, utilities
‚îú‚îÄ‚îÄ app/                  # Streamlit UI and demo interfaces
‚îú‚îÄ‚îÄ notebooks/            # exploratory analysis, prototyping
‚îî‚îÄ‚îÄ scripts/              # CLI tools for training, preprocessing, evaluation
```

Each directory exists for a reason.
The src/model module isolates the GPT architecture so it can be reused across pretraining and fine-tuning tasks. The src/data module handles tokenization, batching and sequence preparation‚Äîthe backbone of any efficient training loop. The CLI tools abstract training into reproducible commands, while the app/ folder provides a simple but complete interface to interact with the model.

Another essential component of the infrastructure is the virtual environment. By isolating dependencies inside a .venv, the project becomes portable and behaves identically on any machine. This also prevents version conflicts, especially with libraries such as PyTorch, which evolve quickly and require careful compatibility control. Installing dependencies through requirements.txt completes this reproducibility cycle.

GPU detection and device routing also form part of the infrastructure layer. The model must gracefully select between CPU, CUDA or Apple Silicon‚Äôs MPS backend depending on what is available. This ensures that both training and inference remain accessible, whether on a high-end workstation or a simple laptop.

At this stage, nothing ‚Äúintelligent‚Äù has happened yet‚Äîbut everything necessary for intelligence to emerge has been prepared. The infrastructure defines how data flows, how models are instantiated, how experiments are executed, and how results are consumed. Without this foundation, even a well-designed architecture would collapse under complexity. With it, the subsequent stages‚Äîtokenization, attention mechanisms, GPT blocks, pretraining, and fine-tuning‚Äîbecome coherent parts of a larger, well-engineered system.

Environment Setup

The project is fully isolated inside a Python virtual environment:

```Python
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

This ensures:
	‚Ä¢	consistent dependencies,
	‚Ä¢	clean execution across machines,
	‚Ä¢	and easy transition into Docker environments (used later for deployment).

Core Components

src/model/

Contains the complete GPT implementation:
	‚Ä¢	multi-head self-attention
	‚Ä¢	feedforward blocks
	‚Ä¢	residual connections
	‚Ä¢	positional embeddings
	‚Ä¢	autoregressive masking
	‚Ä¢	generation utilities

The entire model is written from first principles in pure PyTorch, making it fully transparent and easy to extend.

src/data/

Includes:
	‚Ä¢	the character-level tokenizer
	‚Ä¢	dataset loaders (pretraining, classification, and instruction tuning)
	‚Ä¢	deterministic batching utilities

This abstraction keeps data mechanics independent from model mechanics.

src/finetuning/

Implements:
	‚Ä¢	masked loss computation
	‚Ä¢	instruction-tuning objective
	‚Ä¢	classification head training
	‚Ä¢	freezing/unfreezing strategies

Each finetuning stage reuses the pretrained GPT backbone.

src/cli/

Command-line tools that orchestrate training:

```Python
python -m src.cli.pretrain_char
python -m src.cli.finetune_classification
python -m src.cli.finetune_instructions
```

This modularity allows you to re-run individual stages without recomputing everything.

app/

Houses the Streamlit interface that lets users interact with the tiny LLM:
	‚Ä¢	character-level generation
	‚Ä¢	instruction-based Q&A
	‚Ä¢	FAQ fallback mechanism
	‚Ä¢	model selection (future V2)

This folder connects research to a real UI.

# 2. Working with Text Data: Tokenization & Dataset Preparation.




# 3. Coding Attention Mechanisms.

# 4. GPT Model.


# 5. Pretraining 

# 6. Finetuning for Text Classification (hidden states + backbone preentrenado real)

# 7. Finetuning to Follow Instructions - instruction tuning.

# 8. UI: App Streamlit

# 9. Improvements.

# 10. Examples
 
