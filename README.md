# LLM From Scratch

 ![class](/ima/ima1.png)

---------------------------------------------

**Repository summary**

1.  **Intro** üß≥

This project demonstrates how to build a functional Large Language Model from scratch, starting with raw text data and progressively adding training infrastructure, attention mechanisms, pretraining, instruction tuning, and a UI for real-time interaction. It serves both as an educational journey and a fully reproducible implementation.

2.  **Tech Stack** ü§ñ

The system is built entirely with modern, lightweight tools:

	‚Ä¢	Python 3.10+
	‚Ä¢	PyTorch for model architecture and training
	‚Ä¢	Streamlit for the interactive demo UI
	‚Ä¢	Docker for reproducible deployment
	‚Ä¢	GitHub for version control and documentation
	‚Ä¢	Parquet/Text datasets for training

The project is intentionally minimalistic so every component can be inspected and understood.

3.  **Features** ü§≥üèΩ

‚Ä¢	Character-level tokenizer and dataset pipeline

‚Ä¢	Custom implementation of Multi-Head Self-Attention

‚Ä¢	Compact GPT-style decoder architecture

‚Ä¢	Pretraining on a text corpus

‚Ä¢	Two finetuning pathways:

‚Ä¢	Classification (using frozen backbone + hidden states)

‚Ä¢	Instruction tuning (learning to follow prompts)

‚Ä¢	Streamlit interface with inference + FAQ fallback

‚Ä¢	Dockerized version for portable deployment

4.  **Process** üë£

The project follows a clean, incremental learning path:

1.	Build core infrastructure and modular project structure
2.	Prepare text data and tokenization
3.	Implement attention and transformer blocks
4.	Train a GPT-like model from scratch
5.	Add task-specific finetuning (classification)
6.	Train the model to answer instructions
7.	Deploy an app to interact with the model

Each stage builds on the previous one, making the full pipeline easy to follow.

5.  **Learning** üí°

Throughout the repo, you learn:

‚Ä¢	How transformers actually process data

‚Ä¢	Why attention is computationally expensive

‚Ä¢	How pretraining shapes hidden representations

‚Ä¢	How small models behave vs. large LLMs

‚Ä¢	What instruction tuning requires at a low level

‚Ä¢	How to structure real ML projects for scalability

The goal is not to compete with GPT-size models, but to deeply understand them.

6.  **Improvement** üî©

Future development focuses on:

‚Ä¢	Switching to word/BPE tokenization

‚Ä¢	Scaling model depth and width

‚Ä¢	Expanding the instruction dataset

‚Ä¢	Adding evaluation benchmarks

‚Ä¢	Moving from single-file corpus to multi-domain datasets

‚Ä¢	Migrating to Version 2 with a stronger architecture and training loop

This repository is designed to evolve into a more capable LLM pipeline.

7.  **Running the Project** ‚öôÔ∏è

You can run the project in two ways:

‚Ä¢	Local environment

```Python
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
streamlit run app/streamlit_app.py
```

‚Ä¢	Dockerized deployment

```Python
docker build -t llm-from-scratch-v1 .
docker run -p 8501:8501 llm-from-scratch-v1
```

After launching, open:
http://localhost:8501
to interact with the model.

8 .  **More** üôåüèΩ

For collaboration, discussion, or improvements:

‚Ä¢	GitHub Issues: for bugs or feature requests.

‚Ä¢	Pull Requests: for contributions or new examples.

‚Ä¢	Contact: open an issue or connect via LinkedIn / email (author info in profile).

If this project helps you learn or build better models, consider starring ‚≠ê the repository ‚Äî
it‚Äôs the simplest way to support continued open knowledge sharing.


---------------------------------------------

# :computer: LLM From Scratch  :computer:

---------------------------------------------

# 0. Why build an LLM from Scratch ? 

Building a Large Language Model from scratch is not about competing with industrial-scale systems like GPT-4 or Claude. Instead, it is about understanding‚Äîtruly understanding‚Äîhow modern language models work at the lowest level. When you assemble every component by hand, from tokenization to attention mechanisms to pretraining loops, the architecture stops being a black box and becomes an engineered system whose internal logic you can navigate with confidence.

Most practitioners work with high-level libraries that abstract away the essential mechanics of LLMs. That abstraction is convenient, but it hides the fundamental transformations that give these models their reasoning capabilities: how text becomes numbers, how attention selects and weights context, how residual blocks accumulate knowledge, or how predictions are optimized over billions of tokens. By re-implementing these ideas, you gain a concrete mental model of what is happening at every step of the pipeline.

 ![class](/ima/ima2.png)

This project exists for that purpose. It aims to reconstruct the core pieces of a GPT-style model with clarity and intention, taking nothing for granted. Every function, every shape transformation, and every training step is exposed. You are able to see how a single character or token propagates through embeddings, how attention scores are computed and normalized, how the model predicts the next element in a sequence, and why gradient updates slowly sculpt its internal representation of language.

Once you build an LLM from scratch, you earn three key advantages.
First, you gain the ability to debug and modify any part of the architecture without relying on hidden implementations.
Second, you develop intuition for why certain design decisions‚Äîsuch as positional encodings or normalization layers‚Äîmatter for stability and performance.
And third, you become independent: capable of extending, compressing, adapting or even inventing new architectures instead of depending solely on external frameworks.

This repository is the outcome of that philosophy. It walks through the entire lifecycle of a modern language model‚Äîfrom raw text to a trained model capable of following simple instructions and interacting through a custom interface‚Äîwhile keeping the system transparent and accessible. The result is not a production-ready LLM, but rather a rigorous educational framework: a place where the fundamentals can be learned, inspected, improved and expanded.


# 1. Basic Infraestructure.

Building an LLM from scratch requires a clean, modular, and scalable codebase. The goal is not only to train a model, but to create an environment where experimentation is safe, reproducible, and easy to extend. This project follows a structure inspired by modern ML engineering practices (NLP labs, production AI teams, and open-source frameworks like nanoGPT, PyTorch Lightning, and HuggingFace).

At its core, the infrastructure is designed around four principles:
	1.	Separation of concerns ‚Äî preprocessing logic, model code, training loops, evaluation utilities, and UI demos all live in dedicated modules.
	2.	Reproducibility ‚Äî every step (tokenization, sampling, training configuration) is deterministic and version-controlled.
	3.	Incremental extensibility ‚Äî the project supports a natural evolution:
character-level GPT ‚Üí instruction tuning ‚Üí UI deployment ‚Üí word-level GPT (future V2).
	4.	Ease of experimentation ‚Äî everything can be run via CLI tools or notebooks, without rewriting boilerplate.

The base directory structure is:

```Python
LLM-From-Scratch-Project/
‚îÇ
‚îú‚îÄ‚îÄ data/                 # raw and processed datasets
‚îú‚îÄ‚îÄ models/               # pretrained weights and fine-tuned checkpoints
‚îú‚îÄ‚îÄ src/                  # model architecture, training pipeline, utilities
‚îú‚îÄ‚îÄ app/                  # Streamlit UI and demo interfaces
‚îú‚îÄ‚îÄ notebooks/            # exploratory analysis, prototyping
‚îî‚îÄ‚îÄ scripts/              # CLI tools for training, preprocessing, evaluation
```

Each directory exists for a reason.
The src/model module isolates the GPT architecture so it can be reused across pretraining and fine-tuning tasks. The src/data module handles tokenization, batching and sequence preparation‚Äîthe backbone of any efficient training loop. The CLI tools abstract training into reproducible commands, while the app/ folder provides a simple but complete interface to interact with the model.

Another essential component of the infrastructure is the virtual environment. By isolating dependencies inside a .venv, the project becomes portable and behaves identically on any machine. This also prevents version conflicts, especially with libraries such as PyTorch, which evolve quickly and require careful compatibility control. Installing dependencies through requirements.txt completes this reproducibility cycle.

GPU detection and device routing also form part of the infrastructure layer. The model must gracefully select between CPU, CUDA or Apple Silicon‚Äôs MPS backend depending on what is available. This ensures that both training and inference remain accessible, whether on a high-end workstation or a simple laptop.

At this stage, nothing ‚Äúintelligent‚Äù has happened yet‚Äîbut everything necessary for intelligence to emerge has been prepared. The infrastructure defines how data flows, how models are instantiated, how experiments are executed, and how results are consumed. Without this foundation, even a well-designed architecture would collapse under complexity. With it, the subsequent stages‚Äîtokenization, attention mechanisms, GPT blocks, pretraining, and fine-tuning‚Äîbecome coherent parts of a larger, well-engineered system.

Environment Setup

The project is fully isolated inside a Python virtual environment:

```Python
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

This ensures:
	‚Ä¢	consistent dependencies,
	‚Ä¢	clean execution across machines,
	‚Ä¢	and easy transition into Docker environments (used later for deployment).

Core Components

src/model/

Contains the complete GPT implementation:
	‚Ä¢	multi-head self-attention
	‚Ä¢	feedforward blocks
	‚Ä¢	residual connections
	‚Ä¢	positional embeddings
	‚Ä¢	autoregressive masking
	‚Ä¢	generation utilities

The entire model is written from first principles in pure PyTorch, making it fully transparent and easy to extend.

src/data/

Includes:
	‚Ä¢	the character-level tokenizer
	‚Ä¢	dataset loaders (pretraining, classification, and instruction tuning)
	‚Ä¢	deterministic batching utilities

This abstraction keeps data mechanics independent from model mechanics.

src/finetuning/

Implements:
	‚Ä¢	masked loss computation
	‚Ä¢	instruction-tuning objective
	‚Ä¢	classification head training
	‚Ä¢	freezing/unfreezing strategies

Each finetuning stage reuses the pretrained GPT backbone.

src/cli/

Command-line tools that orchestrate training:

```Python
python -m src.cli.pretrain_char
python -m src.cli.finetune_classification
python -m src.cli.finetune_instructions
```

This modularity allows you to re-run individual stages without recomputing everything.

app/

Houses the Streamlit interface that lets users interact with the tiny LLM:
	‚Ä¢	character-level generation
	‚Ä¢	instruction-based Q&A
	‚Ä¢	FAQ fallback mechanism
	‚Ä¢	model selection (future V2)

This folder connects research to a real UI.

# 2. Working with Text Data: Tokenization & Dataset Preparation.

Before building any language model‚Äîfrom the smallest educational prototype to a large-scale LLM‚Äîthe first challenge is always the same: how to represent text in a way a model can understand.

Version 1 of this project intentionally uses a character-level tokenizer. This is not how modern LLMs operate, but it is the simplest and most transparent way to understand the foundations of autoregressive modeling.

## 2.1 Why Character-Level Tokenization?

For an instructional project, character-level tokenization offers several advantages:

	‚Ä¢	There is no vocabulary to maintain; every visible character becomes a token.
	‚Ä¢	The model never encounters unknown words; it learns to construct them from letters.
	‚Ä¢	The tokenizer is trivial to implement and completely transparent.
	‚Ä¢	It allows you to focus on the mechanics of transformers without relying on external tooling.

Of course, character modeling has limitations: longer sequences, slower convergence, and weaker semantic structure. But it is ideal for understanding how a GPT learns from raw text.

## 2.2 Building the Vocabulary

During preprocessing, the system scans the corpus and extracts every unique character.
Each character is mapped to an integer, forming the vocabulary the model uses.

Alongside the natural characters, several special tokens are added:

	‚Ä¢	a padding token for aligning sequences,
	‚Ä¢	a token for unknown characters,
	‚Ä¢	markers used during instruction tuning (e.g., one to start the instruction and another to start the response).

These special markers allow the same GPT backbone to support pretraining, classification, and instruction-following tasks using a unified token space.

## 2.3 Turning Text Into Model-Ready Sequences

Once the vocabulary exists, every dataset follows the same pipeline:
	1.	Text is converted into a sequence of numerical IDs.
	2.	Sequences are padded or truncated to a fixed maximum length.
	3.	Batches are constructed so the model always receives uniform tensors.

This ensures stability during training and mirrors the requirements of real transformer implementations.


## 2.4 Pretraining Dataset

The backbone GPT is trained using the classic next-token prediction task.
For character-level models, this means predicting the next character in the sequence.

This stage teaches the model:
	‚Ä¢	how text flows,
	‚Ä¢	how characters relate to each other,
	‚Ä¢	how structure emerges from sequential prediction.

Even a tiny model learns patterns such as word boundaries, punctuation, and common character combinations.

## 2.5 Instruction Dataset

For instruction tuning, the data uses simple prompt-response pairs.
Each one is rewritten internally into a structured format that clearly separates:
	‚Ä¢	the instruction,
	‚Ä¢	the model‚Äôs expected response.

This structure helps the small GPT backbone transition from ‚Äúpredict the next character‚Äù to ‚Äúfollow a command,‚Äù even with minimal data.


## 2.6 Summary

This stage lays the groundwork for everything that follows:
	‚Ä¢	A clean and interpretable vocabulary.
	‚Ä¢	A transparent tokenization strategy.
	‚Ä¢	Unified datasets for pretraining, classification, and instruction tuning.
	‚Ä¢	A clear structure for instruction-style interactions.

Even though later versions of the project will shift toward modern tokenization approaches (BPE or WordPiece), V1 deliberately stays simple so every step is visible and understandable.


# 3. Coding attention mechanisms.

Attention is the central innovation that made modern Large Language Models possible.
In this project, the goal was not just to use attention, but to understand it deeply by implementing it from scratch.

Our Version 1 model uses a minimal and educational version of the attention modules found in a GPT architecture. This helps reveal how transformers truly operate behind the scenes.

## 3.1  Motivation: Why Attention?

Traditional sequence models (RNNs, LSTMs, GRUs) struggle with long-range dependencies.
Transformers replaced recurrence with a mechanism that:
	‚Ä¢	looks at all positions in a sequence simultaneously,
	‚Ä¢	weighs how important each token is relative to all others,
	‚Ä¢	and does this efficiently using matrix operations.

This mechanism‚Äîself-attention‚Äîis what allows models like GPT to generate coherent text, learn context, and reason over long spans.

## 3.2 The Core Idea: Queries, Keys, and Values.

Every position in a sequence is transformed into three vectors:
	‚Ä¢	Query (Q) ‚Äì represents the question a token is asking.
	‚Ä¢	Key (K) ‚Äì represents how relevant a token is to others.
	‚Ä¢	Value (V) ‚Äì carries the actual information to be aggregated.

Self-attention computes similarity between each query and key pair.
These similarities determine how much each value contributes to the final representation.

Even at the character level, this mechanism enables the model to:
	‚Ä¢	understand repeated patterns,
	‚Ä¢	capture local and global structure,
	‚Ä¢	build internal representations far richer than raw tokens.

## 3.3 Multi-Head Attention: Learning Multiple Patterns at Once

A single attention head learns only one type of relationship.
GPT uses multiple heads, each focusing on different aspects of the sequence.

In this project:
	‚Ä¢	the number of heads is small,
	‚Ä¢	each head has reduced dimensionality,
	‚Ä¢	and the design follows the original GPT architecture.

Each head analyzes the input differently, and their outputs are combined to produce a more expressive representation.

Even a tiny model benefits from this diversity of viewpoints.

## 3.4 Causal Masking: Enforcing Autoregressive Behavior

GPT models predict the next token.
To ensure the model cannot cheat by looking ahead, the attention mechanism applies a causal mask that blocks future positions.

This ensures the model behaves like a real autoregressive generator:
	‚Ä¢	token t can only attend to positions ‚â§ t,
	‚Ä¢	future characters are never visible during training.

This simple constraint is what makes generation deterministic and left-to-right.

## 3.5 Residual Connections and Layer Normalization

Each attention block is wrapped inside two critical architectural components:
	‚Ä¢	Residual connections, which stabilize learning and allow gradients to flow easily.
	‚Ä¢	Layer normalization, which ensures numerical stability and consistent scaling.

While Version 1 keeps these modules minimal, they replicate the functional structure of production LLM architectures.

## 3.6 Feedforward Network (MLP Block)

Following attention, the transformer includes a positionwise feed-forward network.

In this implementation, it consists of:
	‚Ä¢	a linear expansion,
	‚Ä¢	a non-linear activation,
	‚Ä¢	a linear projection back to the model dimension.

This MLP acts like a local reasoning module, helping the model interpret contextual information and refine its internal representation.
 
## 3.7 Putting It All Together

 A full transformer block in this project includes:
	1.	Multi-head self-attention (with causal masking)
	2.	Residual and normalization layers
	3.	Feedforward MLP
	4.	Another residual and normalization pass

Stacking these blocks produces the GPT backbone used across:
	‚Ä¢	Pretraining
	‚Ä¢	Classification finetuning
	‚Ä¢	Instruction finetuning
	‚Ä¢	Streamlit inference

Even though this model is tiny and character-based, the underlying architecture mirrors that of GPT-2 and GPT-Neo, simply scaled down for educational purposes.

## 3.8  Why Build Attention by Hand?

Implementing these modules manually:

	‚Ä¢	demystifies how LLMs process information,
	‚Ä¢	provides intuition for how patterns emerge,
	‚Ä¢	prepares you for more advanced architectures (GPT-2/3/NeoX),
	‚Ä¢	is the foundation for Version 2, where we transition to word-level or BPE tokenization.

This section is the ‚Äúengine room‚Äù of the project‚Äîunderstanding it means understanding how every modern transformer-based model works internally.

# 4. GPT Model.

The GPT model implemented in this project follows the original decoder-only Transformer design introduced by OpenAI. Although our Version 1 model is intentionally small and character-based, the underlying architecture mirrors the essential structure of real GPT systems.

This section explains how the full model fits together and why each component matters.

## 4.1 A Decoder-Only Transformer

GPT belongs to the family of autoregressive language models:
	‚Ä¢	It receives a sequence of tokens.
	‚Ä¢	It predicts the next token.
	‚Ä¢	It repeats this process step by step.

To support this behavior, the model uses multiple stacked Transformer blocks, each containing:
	1.	Masked Multi-Head Self-Attention
	2.	Feedforward (MLP) sublayer
	3.	Residual connections
	4.	Layer normalization

Even a tiny model benefits from these architectural principles.
They provide stability, expressive power, and the capacity to learn long-range structure.

## 4.2 Token & Position Embeddings

Transformers have no inherent sense of order, so we give them two forms of input representation:

Token embeddings

Every character in the vocabulary is converted into a dense vector.
In Version 1, the model learns to express relationships between characters‚Äîpunctuation, spacing, frequent subpatterns, and more.

Positional embeddings

Since characters appear in sequences, positional embeddings encode the position of each token.
GPT learns how the meaning of a token depends on where it occurs.

Together, token + position embeddings form the input to the first attention block.

## 4.3 Stacked Transformer Blocks

Each GPT block applies the full Transformer logic:
	‚Ä¢	The attention sublayer determines which previous characters are relevant.
	‚Ä¢	The MLP sublayer interprets the aggregated information.
	‚Ä¢	Residual pathways allow information to flow cleanly through the stack.

Increasing the number of blocks increases model depth and reinforces the ability to model patterns.

Although our model is intentionally small, the design aligns with real GPT architectures‚Äîand extending it (more layers, wider embeddings, more heads) becomes straightforward.

## 4.4 Causal Attention: Enforcing Autoregression

GPT is designed to generate text one token at a time.

To enforce this, the attention mechanism includes a strict lower-triangular mask:
	‚Ä¢	A token can attend only to earlier positions.
	‚Ä¢	The model never sees future tokens during training.
	‚Ä¢	Predictions remain consistent with real-world generation.

This property distinguishes GPT from bidirectional models like BERT and is fundamental to sequence generation.

## 4.5 Language Modeling Head (Output Layer)

The final representation of each position passes through a linear projection that maps hidden states to the vocabulary size.

The model outputs a probability distribution over all tokens.
During training, the objective is simple:

Predict the next token in the sequence as accurately as possible.

This is the foundation for:
	‚Ä¢	pretraining on large corpora,
	‚Ä¢	finetuning for classification,
	‚Ä¢	finetuning for instruction following.

Everything else‚Äîthe emergent reasoning, structure, coherence‚Äîarises from maximizing this next-token objective across many examples.

## 4.6 Why Build GPT From Scratch?

Manually implementing GPT demystifies how modern LLMs operate:
	‚Ä¢	You can inspect every tensor and operation.
	‚Ä¢	You understand what is learned and why.
	‚Ä¢	You gain full control over architecture modifications.
	‚Ä¢	You build intuition for scaling laws, training dynamics, and efficiency constraints.

This foundation becomes crucial when transitioning to Version 2, where we incorporate word/BPE tokenization and scale the architecture to handle richer language patterns.

## 4.7 A Minimal but Complete GPT Engine

While Version 1 is small enough to run on a laptop, it includes all essential architectural components:

	‚Ä¢	Learnable embeddings
	‚Ä¢	Multi-Head Self-Attention
	‚Ä¢	Causal masking
	‚Ä¢	Transformer blocks
	‚Ä¢	Autoregressive decoding
	‚Ä¢	Full training loop for pretraining and finetuning

This provides a clean, understandable base that is both educational and extensible.

In short: Version 1 is a real GPT, just scaled down to reveal how everything works.

# 5. Pretraining 

Pretraining is the foundational stage of any GPT-like model. It is where the model learns the fundamental structure of language by repeatedly predicting the next token in a sequence. Even in our small character-level version, this process is what gives the model its initial ‚Äúknowledge‚Äù before any specialized finetuning.

## 5.1 Objective: Next-Token Prediction

The core idea is simple:
	‚Ä¢	Take a long sequence of text.
	‚Ä¢	Feed it into the model.
	‚Ä¢	Ask the model to predict the next character at every position.

Every prediction produces a loss value.
By minimizing that loss across millions of predictions, the model gradually discovers:
	‚Ä¢	which characters or patterns commonly follow others,
	‚Ä¢	how sentences and structures flow,
	‚Ä¢	how context influences meaning.

Even without explicit linguistic rules, the model learns statistical regularities purely from exposure.

This mirrors how real GPT models are trained at scale‚Äîjust with much smaller datasets and capacity.


## 5.2 Training on the OSCAR Corpus

To pretrain the tiny model, we use a subset of the OSCAR dataset‚Äîa large multilingual web corpus.
From it, we derive a simplified Spanish text file (oscar_corpus.txt) suitable for a character-level language model.

Key preprocessing steps:
	‚Ä¢	cleaning non-printable characters
	‚Ä¢	lowercasing or normalization
	‚Ä¢	removing extremely long lines
	‚Ä¢	preparing a contiguous training stream

The model does not learn ‚Äúfacts‚Äù at this stage; it only learns patterns of text.

This is intentional: pretraining provides general linguistic ability, while finetuning injects task-specific knowledge.

## 5.3 Training Dynamics

During pretraining:
	‚Ä¢	The model reads sequences of fixed length.
	‚Ä¢	It shifts the same sequence by one character to build the ‚Äúlabels.‚Äù
	‚Ä¢	The loss function compares the predicted character to the actual next character.
	‚Ä¢	Gradients update all model parameters, including attention, embeddings and MLP layers.

As training progresses, the model develops:
	‚Ä¢	stronger character-level coherence,
	‚Ä¢	better attention focusing on relevant context,
	‚Ä¢	improved ability to complete words or phrases.

Although Version 1 is intentionally tiny, it still demonstrates the essential learning behavior that powers large-scale LLMs.

## 5.4 What Pretraining Enables

Once the model is pretrained, several capabilities emerge:

Generalization

The model can continue text sequences with plausible structure.

Knowledge Transfer

The pretrained backbone serves as a foundation for downstream tasks.
Instead of starting from random weights, finetuning begins from a partially trained model that already ‚Äúunderstands‚Äù syntax and text flow.

Instruction Tuning

In later stages, we teach the model to follow commands using curated prompt ‚Üí response pairs.
Without pretraining, this would be impossible.

## 5.5 Why Pretraining Matters in a From-Scratch Project

Building this stage manually contains immense learning value:
	‚Ä¢	You understand the full data pipeline and why sequence preparation matters.
	‚Ä¢	You see how a Transformer learns from raw text rather than predefined rules.
	‚Ä¢	You experience the computational constraints that guide architecture and dataset choices.
	‚Ä¢	You gain intuition about how scaling up affects behavior and performance.

Pretraining is not just the beginning of the project‚Äîit is the foundation on which every later improvement is built.

## 5.6 Limitations of a Tiny Character-Level Pretrained Model

It is important to acknowledge the constraints:
	‚Ä¢	Character-level modeling is expressive but inefficient.
	‚Ä¢	The model capacity is extremely limited.
	‚Ä¢	Learning semantic knowledge is nearly impossible at this scale.
	‚Ä¢	Sequence length boundaries restrict long-context behaviors.

These limitations motivate the shift to Version 2, where we adopt subword tokenization, longer context windows, and larger model capacity.

# 6. Finetuning for Text Classification (hidden states + backbone preentrenado real)

Once the base model has been pretrained on raw text, it becomes a reusable backbone capable of understanding linguistic patterns. Finetuning leverages this backbone to solve downstream supervised tasks‚Äîin our case, a simple text classification problem designed to show how hidden states can be adapted for real tasks.

This step demonstrates how pretrained representations can be repurposed efficiently, even when the underlying model is tiny.

##  6.1 Why Classification Finetuning Matters

Classification is one of the most common applications of language models:
	‚Ä¢	sentiment analysis
	‚Ä¢	spam detection
	‚Ä¢	topic identification
	‚Ä¢	intent recognition
	‚Ä¢	document routing

Even large-scale models often rely on similar techniques internally.

In our project, classification finetuning serves two purposes:
	1.	Educational ‚Äî it teaches how to adapt a pretrained transformer to a supervised objective.
	2.	Practical ‚Äî it validates that the model‚Äôs learned representations contain usable information beyond simple next-token prediction.

This stage confirms that pretraining truly creates reusable knowledge.

## 6.2 Using the Pretrained Backbone

Instead of training the classifier from scratch, we reuse:
	‚Ä¢	the tokenizer
	‚Ä¢	the embedding layer
	‚Ä¢	the multi-head attention blocks
	‚Ä¢	the feed-forward layers
	‚Ä¢	positional encodings
	‚Ä¢	the overall GPT architecture

All parameters learned during pretraining remain intact.

A lightweight classification head is then added on top of the transformer‚Äôs hidden states, usually:
	‚Ä¢	a small linear layer,
	‚Ä¢	optionally followed by dropout or normalization,
	‚Ä¢	projecting to the number of target classes.

This approach mirrors modern NLP pipelines such as BERT or GPT-style adapters.

## 6.3 Training the Classifier

During finetuning:
	1.	Text samples are fed into the model using the same tokenization as pretraining.
	2.	The transformer produces contextual embeddings for every token.
	3.	A pooled representation (often the final token, or an average) is passed to the classification head.
	4.	The model is optimized using supervised labels and cross-entropy loss.
	5.	Only the classification head may be trained, or optionally the entire backbone may be unfrozen.

Both strategies are valuable:
	‚Ä¢	Frozen backbone ‚Üí fast training, stable results.
	‚Ä¢	Full finetuning ‚Üí higher accuracy but more computational cost.

In our educational setup, either approach is possible.

## 6.4 What the Model Learns

Although the model is small, finetuning enables it to develop:
	‚Ä¢	sensitivity to patterns in the training labels,
	‚Ä¢	ability to classify short inputs reliably,
	‚Ä¢	task-specific decision boundaries based on learned embeddings.

It is remarkable how much performance improvement can arise from a pretrained backbone, even at this tiny scale.

This mirrors the behavior of large models:
pretraining provides general linguistic knowledge, finetuning specializes it.

## 6.5 Interpreting Results

Given the intentionally limited size of our model and dataset, the goal is not cutting-edge accuracy.
Instead, we evaluate:
	‚Ä¢	whether the loss decreases consistently,
	‚Ä¢	whether the classifier separates classes with minimal overfitting,
	‚Ä¢	whether the model relies on meaningful patterns instead of memorization.

A successful finetuning experiment demonstrates that:
	1.	the backbone representations are useful,
	2.	the architecture supports downstream tasks,
	3.	pretraining meaningfully accelerates supervised learning.

## 6.6 Why This Stage Is Crucial for the Full Project

This finetuning step bridges the gap between:
	‚Ä¢	generic language modeling (pretraining), and
	‚Ä¢	task-driven outputs (classification, instruction following).

It also prepares the intuition required for the next stage‚Äîinstruction tuning, where the model transitions from classification to behaving like a conversational assistant.

# 7. Finetuning to Follow Instructions (Instruction Tuning).

After pretraining the model on raw text and validating its backbone through classification, the next major evolutionary step is teaching the model to respond to natural-language instructions.
This is the foundation of modern conversational systems‚ÄîChatGPT, Claude, Gemini, and virtually all instruction-following LLMs rely on this technique.

Instruction tuning transforms a generic language model into an assistant.

## 7.1 What Instruction Tuning Means

A pretrained model knows language patterns, grammar, and relationships between words.
But it does not inherently understand that:

	‚Ä¢	a question expects an answer,
	‚Ä¢	a request expects execution,
	‚Ä¢	a command expects compliance,
	‚Ä¢	or that a prompt defines a task.

Instruction tuning explicitly teaches the model these behaviors.

It does so by exposing it to many examples of the form:

Instruction ‚Üí Ideal Response

This creates alignment between what the user wants and what the model outputs.

Even small models‚Äîlike ours‚Äîbenefit significantly from this approach.

## 7.2 Why We Use a Custom Format (<instr> / <resp>)

Large LLMs use sophisticated prompting conventions, but for a character-level educational model, we need something simple, reproducible, and explicit.

This teaches the model a deterministic conversational protocol:

	1.	When it sees <instr>, it expects a question or command.
	2.	When it sees <resp>, it should begin generating an answer.
	3.	It learns to stop hallucinating metadata and focus on the content.

This structured prompting is critical, especially for tiny models.

## 7.3 Building the Instruction Dataset

Our dataset contains multiple natural variants of the same three facts:

	‚Ä¢	Dogs are canines.
	‚Ä¢	Cats are felines.
	‚Ä¢	San Jos√© is the capital of Costa Rica.

Although small, the dataset covers:

	‚Ä¢	paraphrasing,
	‚Ä¢	variations in phrasing,
	‚Ä¢	alternative sentence structures,
	‚Ä¢	different levels of formality.

This exposes the model to the idea that different inputs can map to the same correct answer.

This concept‚Äîsemantic equivalence‚Äîis central to everything from chatbots to retrieval-augmented models.

## 7.4 Finetuning the Model with Paired Examples

During instruction tuning:

	1.	The pretrained backbone processes the full <instr> ‚Ä¶ <resp> sequence.
	2.	The model is trained to predict the answer text character-by-character.
	3.	It learns to associate instructions with appropriate outputs.
	4.	The optimization objective remains next-token prediction, but conditioned on structured inputs.

This teaches the model not only what to answer, but when and why.

Instruction tuning essentially ‚Äúreshapes‚Äù the probability distribution of the model such that:

	‚Ä¢	Answers become more concise.
	‚Ä¢	Irrelevant generation decreases.
	‚Ä¢	Responses follow user intent.

## 7.5 Limitations of a Tiny Character-Level Model

Even after instruction tuning, our model has structural limitations:

	‚Ä¢	It predicts characters, not words.
	‚Ä¢	It has a very short context window.
	‚Ä¢	Its capacity is orders of magnitude smaller than real LLMs.
	‚Ä¢	It cannot generalize beyond taught facts.

Therefore, we complement the system with a FAQ fallback, ensuring perfect outputs for known queries.

This combination‚Äîtiny LLM + deterministic layer‚Äîis exactly how many production systems handle critical queries.

## 7.6 What We Achieved

By the end of instruction tuning, the model successfully:

	‚Ä¢	recognizes question patterns it never saw verbatim,
	‚Ä¢	answers with consistent factual information,
	‚Ä¢	separates instruction from response,
	‚Ä¢	behaves predictably when interacting with the Streamlit interface.

While small, this stage demonstrates one of the most important ideas in modern AI:

**Pretraining gives the model a brain.

Instruction tuning gives the model a purpose.**

## 7.7 Preparing for the Next Stage

Instruction tuning leads naturally into:

	‚Ä¢	prompt engineering,
	‚Ä¢	evaluation frameworks,
	‚Ä¢	safety layers,
	‚Ä¢	RAG-based augmentation,
	‚Ä¢	multi-task finetuning.

And eventually, the much more ambitious Stage C, where we will replace character-level models with word-level or subword-level transformers, enabling far more realistic capabilities.

# 8. Streamlit UI ‚Äì Building a Minimal LLM Interface.

A language model is most valuable when it becomes interactive‚Äîwhen users can test ideas, ask questions, and explore the model‚Äôs behavior.
To enable this, we built a simple but effective Streamlit-based UI that wraps our tiny LLM into a user-friendly chat-like interface.

Even though the model is small and educational, the frontend reflects the same principles used in real LLM applications.

‚∏ª

## 8.1 Purpose of the UI

The Streamlit interface serves several goals:
	‚Ä¢	Make model behavior observable in a clean, visual environment.
	‚Ä¢	Allow quick experimentation with prompts and generation parameters.
	‚Ä¢	Provide a practical demonstration of instruction tuning and fallback mechanisms.
	‚Ä¢	Enable reproducible testing for the three facts we fine-tuned.

The UI transforms a research artifact into an interactive tool.

‚∏ª

## 8.2 Main Features

The interface includes:

1. Prompt Input Panel

A text area where the user can type or modify any instruction.

2. Predefined Test Questions

A set of radio-button presets for the three core evaluation prompts, making it easy to compare model behavior over time.

3. Generation Controls

Two adjustable parameters exposed in the sidebar:
	‚Ä¢	max_new_tokens
	‚Ä¢	temperature

These allow you to observe how small models behave under different sampling settings‚Äîan essential part of LLM experimentation.

4. Integrated FAQ Fallback (Rule-Based Layer)

Before calling the model, the UI checks whether the input matches one of the known facts.
	‚Ä¢	If it matches ‚Üí A perfect, deterministic answer is returned.
	‚Ä¢	If not ‚Üí The tiny LLM generates a response.

This hybrid pipeline reflects how many production AI systems operate:
models + rules = reliability.

5. Full Generation Trace

The UI displays:
	‚Ä¢	the cleaned, processed answer, and
	‚Ä¢	the entire raw model output, including <instr> and <resp> tokens.

This transparency helps you understand the generation process and diagnose errors.

‚∏ª

## 8.3 Architecture of the UI

The UI sits in the app/ directory and interacts with:
	‚Ä¢	the instruction model loader,
	‚Ä¢	the generation engine,
	‚Ä¢	the tokenizer,
	‚Ä¢	the fallback matcher,
	‚Ä¢	and the Streamlit runtime.

Even though the project is lightweight, the organization reflects a clean separation of concerns:
	‚Ä¢	Frontend logic (Streamlit)
	‚Ä¢	Inference logic (Python modules in src/inference)
	‚Ä¢	Model architecture (in src/model)

This makes it easy to:
	‚Ä¢	externalize the UI into Docker,
	‚Ä¢	replace the model later (e.g., word-level GPT),
	‚Ä¢	add features like temperature sweeps or latency metrics,
	‚Ä¢	integrate logging or analytics.

‚∏ª

## 8.4 Running the Model Locally

The UI was explicitly designed to run without complex infrastructure.
With Python installed, launching the app is as simple as:
	‚Ä¢	activating the local environment,
	‚Ä¢	running Streamlit,
	‚Ä¢	and loading the model checkpoint.

Once launched, the UI runs entirely on your machine and does not require GPUs or external services.

This approach is perfect for educational projects and quick prototyping.

‚∏ª

## 8.5 Dockerized Version

To make the UI reproducible and portable, we containerized the Streamlit application.
The Docker version:
	‚Ä¢	packages all dependencies,
	‚Ä¢	includes the trained checkpoint,
	‚Ä¢	exposes port 8501,
	‚Ä¢	and can be deployed anywhere with Docker installed.

This simulates real-world deployment workflows, where LLM services are often shipped as containerized inference microservices.

‚∏ª

## 8.6 Why This UI Matters

While simple, the Streamlit app is an essential part of the project:
	‚Ä¢	It connects theory with practice.
	‚Ä¢	It allows iterative evaluation as we expand the project (Phase C and beyond).
	‚Ä¢	It demonstrates user-facing LLM interaction even with a tiny model.
	‚Ä¢	It provides a framework for future additions such as:
	‚Ä¢	multi-turn dialogue,
	‚Ä¢	memory,
	‚Ä¢	richer prompt formatting,
	‚Ä¢	RAG-based lookups,
	‚Ä¢	or switching between multiple checkpoints.

The UI makes the LLM real.


# 9. Improvements.

Building an LLM from scratch is an iterative journey.
Version 1 of the project demonstrates the full pipeline‚Äîfrom raw text to a functional (yet tiny) instruction-tuned language model‚Äîbut it also reveals several structural limitations that naturally arise when training small models on limited data.

This section outlines the most impactful improvements planned for the next iterations of the project.
They are grouped into architectural, data-centric, training-centric, and product-centric enhancements.

‚∏ª

## 9.1 Architectural Improvements

1. Move from character-level to word- or subword-level tokenization

The most influential change for V2.

Character-level models are easy to implement but severely limited:
	‚Ä¢	slow to learn long-range dependencies,
	‚Ä¢	inefficient (long sequences ‚Üí high cost),
	‚Ä¢	poor semantic representation capacity,
	‚Ä¢	prone to repetitive or fragmented outputs.

Upgrading to WordPiece, BPE, or SentencePiece fundamentally transforms the model‚Äôs expressiveness.
This alone will produce a dramatic jump in coherence, accuracy, and generalization.

‚∏ª

2. Expand the GPT architecture

Version 1 uses a tiny model for educational clarity.
Version 2 will strengthen it by:
	‚Ä¢	increasing the number of layers,
	‚Ä¢	widening attention heads,
	‚Ä¢	extending embedding sizes,
	‚Ä¢	using RMSNorm or LayerNorm improvements,
	‚Ä¢	adding rotary positional embeddings (RoPE),
	‚Ä¢	exploring FlashAttention for performance.

These enhancements bring the architecture closer to modern small-scale LLMs.

‚∏ª

## 9.2 Data Improvements

1. Build a larger, cleaner corpus

V1 relies on a small locally curated text file.
For meaningful scaling, we will introduce:
	‚Ä¢	more diverse Spanish text sources,
	‚Ä¢	strict deduplication,
	‚Ä¢	document segmentation,
	‚Ä¢	long-context formatting,
	‚Ä¢	heuristic filtering for quality.

Better data = better model.
Even small autoregressive models benefit enormously from thoughtful curation.

‚∏ª

2. Expand the instruction-tuning set

The V1 instruction dataset is intentionally tiny (just enough to demonstrate the pipeline).
Improvements include:
	‚Ä¢	increasing the number of instructions,
	‚Ä¢	diversifying phrasing,
	‚Ä¢	adding different task types (reasoning, extraction, rephrasing, classification),
	‚Ä¢	ensuring balanced examples per instruction family,
	‚Ä¢	adopting self-instruct or synthetic generation techniques.

A richer instruction set enables the model to respond in a more natural, generalizable way.

‚∏ª

## 9.3 Training Pipeline Improvements

1. Better optimization techniques

Future versions will incorporate:

‚Ä¢	cosine learning rate scheduling,
‚Ä¢	warmup steps,
‚Ä¢	gradient clipping,
‚Ä¢	improved weight initialization,
‚Ä¢	mixed-precision training,
‚Ä¢	checkpoint averaging.

These refinements stabilize training and help the model converge to stronger minima.

‚∏ª

2. Scaling the context window

The current model uses a short context (128 tokens).
Version 2 will explore:

‚Ä¢	256, 512, and possibly 1024-token contexts,
	
‚Ä¢	ALiBi or RoPE for scalable positional reasoning.

This massively improves tasks requiring multi-sentence coherence.

‚∏ª

3. More robust validation and evaluation

Rather than relying on a few fixed test questions, we will introduce:

	‚Ä¢	automated evaluation suites,
	‚Ä¢	perplexity tracking,
	‚Ä¢	held-out datasets,
	‚Ä¢	instruction-based benchmarks.

This transforms the project from ‚Äúdoes it run?‚Äù to ‚Äúhow well does it perform?‚Äù.

## 9.4 Product and Interface Improvements

1. Improve the Streamlit UI

Planned additions:

	‚Ä¢	chat-style conversation history,
	‚Ä¢	adjustable system prompts,
	‚Ä¢	token usage visualization,
	‚Ä¢	latency monitoring,
	‚Ä¢	multiple model checkpoints selection.

These features bring the interface closer to a real LLM playground.

2. API layer for deployment

To support external integrations, a FastAPI inference service is planned:

	‚Ä¢	clean REST endpoints,
	‚Ä¢	lightweight load balancing,
	‚Ä¢	Docker-ready deployment,
	‚Ä¢	option to run CPU/GPU versions.

This bridges the project into more production-oriented territory.

3. Modularity for experimenting with different models

The repository will support ‚Äúplug-and-play‚Äù models:

	‚Ä¢	character-level GPT (current V1)
	‚Ä¢	word-level GPT (V2)
	‚Ä¢	distilled versions
	‚Ä¢	quantized versions (int8, int4)

This modularity is key for ongoing research and experimentation.

‚∏ª

## 9.5 Vision for V2 and Beyond

The aim of these improvements is not just incremental refinement‚Äîit is to transform the project from a proof of concept into a mini research playground for LLM construction.

Version 2 will represent:

‚Ä¢	a more modern architecture,

‚Ä¢	a more realistic dataset pipeline,

‚Ä¢	a more expressive tokenizer,

‚Ä¢	a more capable model,

‚Ä¢	and a significantly more powerful instruction-tuned system.

And because every component remains fully transparent and hand-built, the educational value increases rather than decreases.

# 10. Examples

This section showcases a few screenshots of the working application built in Version 1.
The images illustrate how the Streamlit interface loads the instruction-tuned model, processes user questions, and combines both inference and fallback logic to provide responses.

These examples demonstrate the end-to-end functionality of the system:
from raw text ‚Üí pretrained model ‚Üí instruction tuning ‚Üí interactive UI.
