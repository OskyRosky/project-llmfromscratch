# app/streamlit_app.py

import os
import sys

import streamlit as st

# ---------------------------------------------------------------------
# Asegurar que podamos hacer "import src..."
# ---------------------------------------------------------------------
ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if ROOT not in sys.path:
    sys.path.append(ROOT)

from src.inference.instructions_chat import (  # noqa: E402
    load_instructions_model,
    generate_answer,
    InstructionsModelBundle,
)

# ---------------------------------------------------------------------
# Configuraci√≥n b√°sica de la p√°gina
# ---------------------------------------------------------------------
st.set_page_config(
    page_title="LLM From Scratch - Instruction Chat",
    page_icon="üí¨",
    layout="wide",
)


# ---------------------------------------------------------------------
# Funci√≥n cacheada para cargar el modelo UNA sola vez
# ---------------------------------------------------------------------
@st.cache_resource(show_spinner="Cargando modelo (solo la primera vez)...")
def get_model_bundle(device_str: str = "mps") -> InstructionsModelBundle:
    """
    Carga el modelo de instrucciones y lo cachea.

    OJO:
      - Aqu√≠ usamos la MISMA interfaz que en eval_instructions_mini:
        load_instructions_model(ckpt_dir, device_str=...)
    """
    ckpt_dir = "models/checkpoints_oscar_long"

    st.write(f"[DEBUG] Cargando modelo en dispositivo: {device_str}")
    bundle = load_instructions_model(
        ckpt_dir=ckpt_dir,
        device_str=device_str,
    )
    return bundle


# ---------------------------------------------------------------------
# Sidebar: par√°metros de generaci√≥n
# ---------------------------------------------------------------------
st.sidebar.header("‚öôÔ∏è Par√°metros de generaci√≥n")

max_new_tokens = st.sidebar.slider(
    "max_new_tokens",
    min_value=10,
    max_value=200,
    value=80,
    step=5,
)

temperature = st.sidebar.slider(
    "temperature",
    min_value=0.0,
    max_value=1.5,
    value=0.0,  # igual que en eval_instructions_mini para que sea determinista
    step=0.05,
)

st.sidebar.markdown("---")
st.sidebar.markdown(
    "Backend: `gpt_char_instructions.pt` en "
    "`models/checkpoints_oscar_long/`."
)


# ---------------------------------------------------------------------
# Layout principal
# ---------------------------------------------------------------------
st.title("üí¨ LLM From Scratch ‚Äì Instruction Chat (tiny)")

st.markdown(
    """
Modelo **car√°cter a car√°cter** entrenado desde cero sobre `oscar_corpus.txt`  
y luego *instruction-tuned* con un conjunto m√≠nimo de pares (instrucci√≥n ‚Üí respuesta).

‚ö†Ô∏è **Este modelo es muy peque√±o y educativo**, no esperes respuestas tipo ChatGPT.
"""
)

st.markdown("---")

st.markdown("### Pregunta de prueba")

opciones = [
    "Los perros son caninos?",
    "Los gatos son felinos?",
    "Cu√°l es la capital de Costa Rica?",
]

pregunta_base = st.radio(
    "Elige una de las preguntas de test:",
    opciones,
    index=0,
)

prompt = st.text_area(
    "Puedes ajustar la pregunta si quieres:",
    value=pregunta_base,
    height=100,
)

if st.button("Generar respuesta"):
    if not prompt.strip():
        st.warning("Por favor escribe una instrucci√≥n o pregunta.")
    else:
        with st.spinner("Cargando modelo (si es la primera vez) y generando respuesta..."):
            # Cargamos el bundle SOLO aqu√≠ (y cacheado)
            bundle = get_model_bundle(device_str="mps")

            # IMPORTANTE: generate_answer ya la tienes retornando (answer_text, full_text)
            answer_text, full_text = generate_answer(
                bundle=bundle,
                prompt=prompt,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
            )

        st.markdown("### üü¢ Respuesta procesada (solo despu√©s de `<resp>`)")
        st.write(answer_text)

        st.markdown("### üìú Texto completo generado")
        st.code(repr(full_text), language="python")

        st.markdown("---")
        st.markdown(
            "_Recuerda: este es un modelo tiny para fines educativos; "
            "las respuestas pueden ser incoherentes._"
        )
else:
    st.info("Elige una pregunta y pulsa **Generar respuesta**.")